--- hdbscan/hdbscan/_hdbscan_linkage.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_linkage.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,34 +1,35 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
 # Minimum spanning tree single linkage implementation for hdbscan
 # Authors: Leland McInnes, Steve Astels
 # License: 3-clause BSD
 
 import numpy as np
-cimport numpy as np
+cimport numpy as cnp
+import cython
 
 from libc.float cimport DBL_MAX
+from libc.stdio cimport printf
 
-from dist_metrics cimport DistanceMetric
+from sklearn.metrics._dist_metrics cimport DistanceMetric
 
 
-cpdef np.ndarray[np.double_t, ndim=2] mst_linkage_core(
-                               np.ndarray[np.double_t,
-                                          ndim=2] distance_matrix):
-
-    cdef np.ndarray[np.intp_t, ndim=1] node_labels
-    cdef np.ndarray[np.intp_t, ndim=1] current_labels
-    cdef np.ndarray[np.double_t, ndim=1] current_distances
-    cdef np.ndarray[np.double_t, ndim=1] left
-    cdef np.ndarray[np.double_t, ndim=1] right
-    cdef np.ndarray[np.double_t, ndim=2] result
-
-    cdef np.ndarray label_filter
-
-    cdef np.intp_t current_node
-    cdef np.intp_t new_node_index
-    cdef np.intp_t new_node
-    cdef np.intp_t i
+cpdef cnp.ndarray[cnp.double_t, ndim=2] mst_linkage_core(
+    cnp.ndarray[cnp.double_t, ndim=2] distance_matrix
+):
+
+    cdef:
+        cnp.ndarray[cnp.intp_t, ndim=1] node_labels
+        cnp.ndarray[cnp.intp_t, ndim=1] current_labels
+        cnp.ndarray[cnp.double_t, ndim=1] current_distances
+        cnp.ndarray[cnp.double_t, ndim=1] left
+        cnp.ndarray[cnp.double_t, ndim=1] right
+        cnp.ndarray[cnp.double_t, ndim=2] result
+
+        cnp.ndarray label_filter
+
+        cnp.intp_t current_node
+        cnp.intp_t new_node_index
+        cnp.intp_t new_node
+        cnp.intp_t i
 
     result = np.zeros((distance_matrix.shape[0] - 1, 3))
     node_labels = np.arange(distance_matrix.shape[0], dtype=np.intp)
@@ -52,50 +53,51 @@
     return result
 
 
-cpdef np.ndarray[np.double_t, ndim=2] mst_linkage_core_vector(
-        np.ndarray[np.double_t, ndim=2, mode='c'] raw_data,
-        np.ndarray[np.double_t, ndim=1, mode='c'] core_distances,
-        DistanceMetric dist_metric,
-        np.double_t alpha=1.0):
-
-    # Add a comment
-    cdef np.ndarray[np.double_t, ndim=1] current_distances_arr
-    cdef np.ndarray[np.double_t, ndim=1] current_sources_arr
-    cdef np.ndarray[np.int8_t, ndim=1] in_tree_arr
-    cdef np.ndarray[np.double_t, ndim=2] result_arr
-
-    cdef np.double_t * current_distances
-    cdef np.double_t * current_sources
-    cdef np.double_t * current_core_distances
-    cdef np.double_t * raw_data_ptr
-    cdef np.int8_t * in_tree
-    cdef np.double_t[:, ::1] raw_data_view
-    cdef np.double_t[:, ::1] result
-
-    cdef np.ndarray label_filter
-
-    cdef np.intp_t current_node
-    cdef np.intp_t source_node
-    cdef np.intp_t right_node
-    cdef np.intp_t left_node
-    cdef np.intp_t new_node
-    cdef np.intp_t i
-    cdef np.intp_t j
-    cdef np.intp_t dim
-    cdef np.intp_t num_features
-
-    cdef double current_node_core_distance
-    cdef double right_value
-    cdef double left_value
-    cdef double core_value
-    cdef double new_distance
+cpdef cnp.ndarray[cnp.double_t, ndim=2] mst_linkage_core_vector(
+    cnp.ndarray[cnp.double_t, ndim=2, mode='c'] raw_data,
+    cnp.ndarray[cnp.double_t, ndim=1, mode='c'] core_distances,
+    DistanceMetric dist_metric,
+    cnp.double_t alpha=1.0
+):
+
+    cdef:
+        cnp.ndarray[cnp.double_t, ndim=1] current_distances_arr
+        cnp.ndarray[cnp.double_t, ndim=1] current_sources_arr
+        cnp.ndarray[cnp.int8_t, ndim=1] in_tree_arr
+        cnp.ndarray[cnp.double_t, ndim=2] result_arr
+
+        cnp.double_t * current_distances
+        cnp.double_t * current_sources
+        cnp.double_t * current_core_distances
+        cnp.double_t * raw_data_ptr
+        cnp.int8_t * in_tree
+        cnp.double_t[:, ::1] raw_data_view
+        cnp.double_t[:, ::1] result
+
+        cnp.ndarray label_filter
+
+        cnp.intp_t current_node
+        cnp.intp_t source_node
+        cnp.intp_t right_node
+        cnp.intp_t left_node
+        cnp.intp_t new_node
+        cnp.intp_t i
+        cnp.intp_t j
+        cnp.intp_t dim
+        cnp.intp_t num_features
+
+        double current_node_core_distance
+        double right_value
+        double left_value
+        double core_value
+        double new_distance
 
     dim = raw_data.shape[0]
     num_features = raw_data.shape[1]
 
-    raw_data_view = (<np.double_t[:raw_data.shape[0], :raw_data.shape[1]:1]> (
-        <np.double_t *> raw_data.data))
-    raw_data_ptr = (<np.double_t *> &raw_data_view[0, 0])
+    raw_data_view = (<cnp.double_t[:raw_data.shape[0], :raw_data.shape[1]:1]> (
+        <cnp.double_t *> raw_data.data))
+    raw_data_ptr = (<cnp.double_t *> &raw_data_view[0, 0])
 
     result_arr = np.zeros((dim - 1, 3))
     in_tree_arr = np.zeros(dim, dtype=np.int8)
@@ -103,11 +105,11 @@
     current_distances_arr = np.infty * np.ones(dim)
     current_sources_arr = np.ones(dim)
 
-    result = (<np.double_t[:dim - 1, :3:1]> (<np.double_t *> result_arr.data))
-    in_tree = (<np.int8_t *> in_tree_arr.data)
-    current_distances = (<np.double_t *> current_distances_arr.data)
-    current_sources = (<np.double_t *> current_sources_arr.data)
-    current_core_distances = (<np.double_t *> core_distances.data)
+    result = (<cnp.double_t[:dim - 1, :3:1]> (<cnp.double_t *> result_arr.data))
+    in_tree = (<cnp.int8_t *> in_tree_arr.data)
+    current_distances = (<cnp.double_t *> current_distances_arr.data)
+    current_sources = (<cnp.double_t *> current_sources_arr.data)
+    current_core_distances = (<cnp.double_t *> core_distances.data)
 
     for i in range(1, dim):
 
@@ -175,21 +177,22 @@
 
 cdef class UnionFind (object):
 
-    cdef np.ndarray parent_arr
-    cdef np.ndarray size_arr
-    cdef np.intp_t next_label
-    cdef np.intp_t *parent
-    cdef np.intp_t *size
+    cdef:
+        cnp.ndarray parent_arr
+        cnp.ndarray size_arr
+        cnp.intp_t next_label
+        cnp.intp_t *parent
+        cnp.intp_t *size
 
     def __init__(self, N):
         self.parent_arr = -1 * np.ones(2 * N - 1, dtype=np.intp, order='C')
         self.next_label = N
         self.size_arr = np.hstack((np.ones(N, dtype=np.intp),
                                    np.zeros(N-1, dtype=np.intp)))
-        self.parent = (<np.intp_t *> self.parent_arr.data)
-        self.size = (<np.intp_t *> self.size_arr.data)
+        self.parent = (<cnp.intp_t *> self.parent_arr.data)
+        self.size = (<cnp.intp_t *> self.size_arr.data)
 
-    cdef void union(self, np.intp_t m, np.intp_t n):
+    cdef void union(self, cnp.intp_t m, cnp.intp_t n):
         self.size[self.next_label] = self.size[m] + self.size[n]
         self.parent[m] = self.next_label
         self.parent[n] = self.next_label
@@ -198,8 +201,9 @@
 
         return
 
-    cdef np.intp_t fast_find(self, np.intp_t n):
-        cdef np.intp_t p
+    @cython.wraparound(True)
+    cdef cnp.intp_t fast_find(self, cnp.intp_t n):
+        cdef cnp.intp_t p
         p = n
         while self.parent_arr[n] != -1:
             n = self.parent_arr[n]
@@ -208,25 +212,26 @@
             p, self.parent_arr[p] = self.parent_arr[p], n
         return n
 
+@cython.wraparound(True)
+cpdef cnp.ndarray[cnp.double_t, ndim=2] label(cnp.double_t[:,:] L):
 
-cpdef np.ndarray[np.double_t, ndim=2] label(np.ndarray[np.double_t, ndim=2] L):
+    cdef:
+        cnp.ndarray[cnp.double_t, ndim=2] result_arr
+        cnp.double_t[:, ::1] result
 
-    cdef np.ndarray[np.double_t, ndim=2] result_arr
-    cdef np.double_t[:, ::1] result
-
-    cdef np.intp_t N, a, aa, b, bb, index
-    cdef np.double_t delta
+        cnp.intp_t N, a, aa, b, bb, index
+        cnp.double_t delta
 
     result_arr = np.zeros((L.shape[0], L.shape[1] + 1))
-    result = (<np.double_t[:L.shape[0], :4:1]> (
-        <np.double_t *> result_arr.data))
+    result = (<cnp.double_t[:L.shape[0], :4:1]> (
+        <cnp.double_t *> result_arr.data))
     N = L.shape[0] + 1
     U = UnionFind(N)
 
     for index in range(L.shape[0]):
 
-        a = <np.intp_t> L[index, 0]
-        b = <np.intp_t> L[index, 1]
+        a = <cnp.intp_t> L[index, 0]
+        b = <cnp.intp_t> L[index, 1]
         delta = L[index, 2]
 
         aa, bb = U.fast_find(a), U.fast_find(b)
@@ -239,14 +244,3 @@
         U.union(aa, bb)
 
     return result_arr
-
-
-cpdef np.ndarray[np.double_t, ndim=2] single_linkage(distance_matrix):
-
-    cdef np.ndarray[np.double_t, ndim=2] hierarchy
-    cdef np.ndarray[np.double_t, ndim=2] for_labelling
-
-    hierarchy = mst_linkage_core(distance_matrix)
-    for_labelling = hierarchy[np.argsort(hierarchy.T[2]), :]
-
-    return label(for_labelling)
--- hdbscan/hdbscan/_hdbscan_reachability.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_reachability.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,20 +1,20 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
-# cython: initializedcheck=False
 # mutual reachability distance compiutations
 # Authors: Leland McInnes
 # License: 3-clause BSD
 
 import numpy as np
+
 cimport numpy as np
 
-from scipy.spatial.distance import pdist, squareform
-from scipy.sparse import lil_matrix as sparse_matrix
-from sklearn.neighbors import KDTree, BallTree
 import gc
 
+from scipy.sparse import lil_matrix as sparse_matrix
+from scipy.spatial.distance import pdist, squareform
 
-def mutual_reachability(distance_matrix, min_points=5, alpha=1.0):
+from sklearn.neighbors import BallTree, KDTree
+
+
+def mutual_reachability(distance_matrix, min_points=5, alpha=None):
     """Compute the weighted adjacency matrix of the mutual reachability
     graph of a distance matrix.
 
@@ -23,10 +23,14 @@
     distance_matrix : ndarray, shape (n_samples, n_samples)
         Array of distances between samples.
 
-    min_points : int, optional (default=5)
+    min_points : int, default=5
         The number of points in a neighbourhood for a point to be considered
         a core point.
 
+    alpha : float, default=None
+        A distance scaling parameter as used in robust single linkage. This
+        divides the distances when calculating mutual reachability.
+
     Returns
     -------
     mututal_reachability: ndarray, shape (n_samples, n_samples)
@@ -49,7 +53,7 @@
         core_distances = np.sort(distance_matrix,
                                  axis=0)[min_points]
 
-    if alpha != 1.0:
+    if alpha is not None:
         distance_matrix = distance_matrix / alpha
 
     stage1 = np.where(core_distances > distance_matrix,
@@ -97,117 +101,3 @@
             result[i, j] = max_dist
 
     return result.tocsr()
-
-
-def kdtree_mutual_reachability(X, distance_matrix, metric, p=2, min_points=5,
-                               alpha=1.0, **kwargs):
-    dim = distance_matrix.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    if metric == 'minkowski':
-        tree = KDTree(X, metric=metric, p=p)
-    else:
-        tree = KDTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    if alpha != 1.0:
-        distance_matrix = distance_matrix / alpha
-
-    stage1 = np.where(core_distances > distance_matrix,
-                      core_distances, distance_matrix)
-    result = np.where(core_distances > stage1.T,
-                      core_distances.T, stage1.T).T
-    return result
-
-
-def balltree_mutual_reachability(X, distance_matrix, metric, p=2, min_points=5,
-                                 alpha=1.0, **kwargs):
-    dim = distance_matrix.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    tree = BallTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    if alpha != 1.0:
-        distance_matrix = distance_matrix / alpha
-
-    stage1 = np.where(core_distances > distance_matrix,
-                      core_distances, distance_matrix)
-    result = np.where(core_distances > stage1.T,
-                      core_distances.T, stage1.T).T
-    return result
-
-
-cdef np.ndarray[np.double_t, ndim=1] mutual_reachability_from_pdist(
-        np.ndarray[np.double_t, ndim=1] core_distances,
-        np.ndarray[np.double_t, ndim=1] dists, np.intp_t dim):
-
-    cdef np.intp_t i
-    cdef np.intp_t j
-    cdef np.intp_t result_pos
-
-    result_pos = 0
-    for i in range(dim):
-        for j in range(i + 1, dim):
-            if core_distances[i] > core_distances[j]:
-                if core_distances[i] > dists[result_pos]:
-                    dists[result_pos] = core_distances[i]
-
-            else:
-                if core_distances[j] > dists[result_pos]:
-                    dists[result_pos] = core_distances[j]
-
-            result_pos += 1
-
-    return dists
-
-
-def kdtree_pdist_mutual_reachability(X,  metric, p=2, min_points=5, alpha=1.0,
-                                     **kwargs):
-
-    dim = X.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    if metric == 'minkowski':
-        tree = KDTree(X, metric=metric, p=p)
-    else:
-        tree = KDTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    del tree
-    gc.collect()
-
-    dists = pdist(X, metric=metric, p=p, **kwargs)
-
-    if alpha != 1.0:
-        dists /= alpha
-
-    dists = mutual_reachability_from_pdist(core_distances, dists, dim)
-
-    return dists
-
-
-def balltree_pdist_mutual_reachability(X, metric, p=2, min_points=5, alpha=1.0,
-                                       **kwargs):
-
-    dim = X.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    tree = BallTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    del tree
-    gc.collect()
-
-    dists = pdist(X, metric=metric, p=p, **kwargs)
-
-    if alpha != 1.0:
-        dists /= alpha
-
-    dists = mutual_reachability_from_pdist(core_distances, dists, dim)
-
-    return dists
--- hdbscan/hdbscan/_hdbscan_tree.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_tree.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,13 +1,14 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
-# cython: initializedcheck=False
 # Tree handling (condensing, finding stable clusters) for hdbscan
 # Authors: Leland McInnes
 # License: 3-clause BSD
 
 import numpy as np
+
 cimport numpy as np
 
+import cython
+
+
 cdef np.double_t INFTY = np.inf
 
 
@@ -295,7 +296,7 @@
             # Initialize
             current_parent = parent
             max_lambda = lambda_
-    
+
     deaths[current_parent] = max_lambda # value for last parent
 
     return deaths_arr
@@ -420,8 +421,7 @@
         set clusters,
         dict cluster_label_map,
         np.intp_t allow_single_cluster,
-        np.double_t cluster_selection_epsilon,
-        np.intp_t match_reference_implementation):
+        np.double_t cluster_selection_epsilon):
 
     cdef np.intp_t root_cluster
     cdef np.ndarray[np.intp_t, ndim=1] result_arr
@@ -470,15 +470,7 @@
             else:
                 result[n] = -1
         else:
-            if match_reference_implementation:
-                point_lambda = lambda_array[child_array == n][0]
-                cluster_lambda = lambda_array[child_array == cluster][0]
-                if point_lambda > cluster_lambda:
-                    result[n] = cluster_label_map[cluster]
-                else:
-                    result[n] = -1
-            else:
-                result[n] = cluster_label_map[cluster]
+            result[n] = cluster_label_map[cluster]
 
     return result_arr
 
@@ -527,85 +519,6 @@
     return result
 
 
-cpdef np.ndarray[np.double_t, ndim=1] outlier_scores(np.ndarray tree):
-    """Generate GLOSH outlier scores from a condensed tree.
-
-    Parameters
-    ----------
-    tree : numpy recarray
-        The condensed tree to generate GLOSH outlier scores from
-
-    Returns
-    -------
-    outlier_scores : ndarray (n_samples,)
-        Outlier scores for each sample point. The larger the score
-        the more outlying the point.
-    """
-
-    cdef np.ndarray[np.double_t, ndim=1] result
-    cdef np.ndarray[np.double_t, ndim=1] deaths
-    cdef np.ndarray[np.double_t, ndim=1] lambda_array
-    cdef np.ndarray[np.intp_t, ndim=1] child_array
-    cdef np.ndarray[np.intp_t, ndim=1] parent_array
-    cdef np.intp_t root_cluster
-    cdef np.intp_t point
-    cdef np.intp_t parent
-    cdef np.intp_t cluster
-    cdef np.double_t lambda_max
-
-    child_array = tree['child']
-    parent_array = tree['parent']
-    lambda_array = tree['lambda_val']
-
-    deaths = max_lambdas(tree)
-    root_cluster = parent_array.min()
-    result = np.zeros(root_cluster, dtype=np.double)
-
-    topological_sort_order = np.argsort(parent_array)
-    # topologically_sorted_tree = tree[topological_sort_order]
-
-    for n in topological_sort_order:
-        cluster = child_array[n]
-        if cluster < root_cluster:
-            break
-
-        parent = parent_array[n]
-        if deaths[cluster] > deaths[parent]:
-            deaths[parent] = deaths[cluster]
-
-    for n in range(tree.shape[0]):
-        point = child_array[n]
-        if point >= root_cluster:
-            continue
-
-        cluster = parent_array[n]
-        lambda_max = deaths[cluster]
-
-
-        if lambda_max == 0.0 or not np.isfinite(lambda_array[n]):
-            result[point] = 0.0
-        else:
-            result[point] = (lambda_max - lambda_array[n]) / lambda_max
-
-    return result
-
-
-cpdef np.ndarray get_stability_scores(np.ndarray labels, set clusters,
-                                      dict stability, np.double_t max_lambda):
-
-    cdef np.intp_t cluster_size
-    cdef np.intp_t n
-
-    result = np.empty(len(clusters), dtype=np.double)
-    for n, c in enumerate(sorted(list(clusters))):
-        cluster_size = np.sum(labels == n)
-        if np.isinf(max_lambda) or max_lambda == 0.0 or cluster_size == 0:
-            result[n] = 1.0
-        else:
-            result[n] = stability[c] / (cluster_size * max_lambda)
-
-    return result
-
 cpdef list recurse_leaf_dfs(np.ndarray cluster_tree, np.intp_t current_node):
     children = cluster_tree[cluster_tree['parent'] == current_node]['child']
     if len(children) == 0:
@@ -656,10 +569,10 @@
 
     return set(selected_clusters)
 
+@cython.wraparound(True)
 cpdef tuple get_clusters(np.ndarray tree, dict stability,
                          cluster_selection_method='eom',
                          allow_single_cluster=False,
-                         match_reference_implementation=False,
                          cluster_selection_epsilon=0.0,
                          max_cluster_size=0):
     """Given a tree and stability dict, produce the cluster labels
@@ -683,13 +596,9 @@
         Whether to allow a single cluster to be selected by the
         Excess of Mass algorithm.
 
-    match_reference_implementation : boolean, optional (default False)
-        Whether to match the reference implementation in how to handle
-        certain edge cases.
-
     cluster_selection_epsilon: float, optional (default 0.0)
         A distance threshold for cluster splits.
-        
+
     max_cluster_size: int, optional (default 0)
         The maximum size for clusters located by the EOM clusterer. Can
         be overridden by the cluster_selection_epsilon parameter in
@@ -798,9 +707,7 @@
     reverse_cluster_map = {n: c for c, n in cluster_map.items()}
 
     labels = do_labelling(tree, clusters, cluster_map,
-                          allow_single_cluster, cluster_selection_epsilon,
-                          match_reference_implementation)
+                          allow_single_cluster, cluster_selection_epsilon)
     probs = get_probabilities(tree, reverse_cluster_map, labels)
-    stabilities = get_stability_scores(labels, clusters, stability, max_lambda)
 
-    return (labels, probs, stabilities)
+    return (labels, probs)
--- hdbscan/hdbscan/_hdbscan_linkage.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_linkage.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,34 +1,35 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
 # Minimum spanning tree single linkage implementation for hdbscan
 # Authors: Leland McInnes, Steve Astels
 # License: 3-clause BSD
 
 import numpy as np
-cimport numpy as np
+cimport numpy as cnp
+import cython
 
 from libc.float cimport DBL_MAX
+from libc.stdio cimport printf
 
-from dist_metrics cimport DistanceMetric
+from sklearn.metrics._dist_metrics cimport DistanceMetric
 
 
-cpdef np.ndarray[np.double_t, ndim=2] mst_linkage_core(
-                               np.ndarray[np.double_t,
-                                          ndim=2] distance_matrix):
-
-    cdef np.ndarray[np.intp_t, ndim=1] node_labels
-    cdef np.ndarray[np.intp_t, ndim=1] current_labels
-    cdef np.ndarray[np.double_t, ndim=1] current_distances
-    cdef np.ndarray[np.double_t, ndim=1] left
-    cdef np.ndarray[np.double_t, ndim=1] right
-    cdef np.ndarray[np.double_t, ndim=2] result
-
-    cdef np.ndarray label_filter
-
-    cdef np.intp_t current_node
-    cdef np.intp_t new_node_index
-    cdef np.intp_t new_node
-    cdef np.intp_t i
+cpdef cnp.ndarray[cnp.double_t, ndim=2] mst_linkage_core(
+    cnp.ndarray[cnp.double_t, ndim=2] distance_matrix
+):
+
+    cdef:
+        cnp.ndarray[cnp.intp_t, ndim=1] node_labels
+        cnp.ndarray[cnp.intp_t, ndim=1] current_labels
+        cnp.ndarray[cnp.double_t, ndim=1] current_distances
+        cnp.ndarray[cnp.double_t, ndim=1] left
+        cnp.ndarray[cnp.double_t, ndim=1] right
+        cnp.ndarray[cnp.double_t, ndim=2] result
+
+        cnp.ndarray label_filter
+
+        cnp.intp_t current_node
+        cnp.intp_t new_node_index
+        cnp.intp_t new_node
+        cnp.intp_t i
 
     result = np.zeros((distance_matrix.shape[0] - 1, 3))
     node_labels = np.arange(distance_matrix.shape[0], dtype=np.intp)
@@ -52,50 +53,51 @@
     return result
 
 
-cpdef np.ndarray[np.double_t, ndim=2] mst_linkage_core_vector(
-        np.ndarray[np.double_t, ndim=2, mode='c'] raw_data,
-        np.ndarray[np.double_t, ndim=1, mode='c'] core_distances,
-        DistanceMetric dist_metric,
-        np.double_t alpha=1.0):
-
-    # Add a comment
-    cdef np.ndarray[np.double_t, ndim=1] current_distances_arr
-    cdef np.ndarray[np.double_t, ndim=1] current_sources_arr
-    cdef np.ndarray[np.int8_t, ndim=1] in_tree_arr
-    cdef np.ndarray[np.double_t, ndim=2] result_arr
-
-    cdef np.double_t * current_distances
-    cdef np.double_t * current_sources
-    cdef np.double_t * current_core_distances
-    cdef np.double_t * raw_data_ptr
-    cdef np.int8_t * in_tree
-    cdef np.double_t[:, ::1] raw_data_view
-    cdef np.double_t[:, ::1] result
-
-    cdef np.ndarray label_filter
-
-    cdef np.intp_t current_node
-    cdef np.intp_t source_node
-    cdef np.intp_t right_node
-    cdef np.intp_t left_node
-    cdef np.intp_t new_node
-    cdef np.intp_t i
-    cdef np.intp_t j
-    cdef np.intp_t dim
-    cdef np.intp_t num_features
-
-    cdef double current_node_core_distance
-    cdef double right_value
-    cdef double left_value
-    cdef double core_value
-    cdef double new_distance
+cpdef cnp.ndarray[cnp.double_t, ndim=2] mst_linkage_core_vector(
+    cnp.ndarray[cnp.double_t, ndim=2, mode='c'] raw_data,
+    cnp.ndarray[cnp.double_t, ndim=1, mode='c'] core_distances,
+    DistanceMetric dist_metric,
+    cnp.double_t alpha=1.0
+):
+
+    cdef:
+        cnp.ndarray[cnp.double_t, ndim=1] current_distances_arr
+        cnp.ndarray[cnp.double_t, ndim=1] current_sources_arr
+        cnp.ndarray[cnp.int8_t, ndim=1] in_tree_arr
+        cnp.ndarray[cnp.double_t, ndim=2] result_arr
+
+        cnp.double_t * current_distances
+        cnp.double_t * current_sources
+        cnp.double_t * current_core_distances
+        cnp.double_t * raw_data_ptr
+        cnp.int8_t * in_tree
+        cnp.double_t[:, ::1] raw_data_view
+        cnp.double_t[:, ::1] result
+
+        cnp.ndarray label_filter
+
+        cnp.intp_t current_node
+        cnp.intp_t source_node
+        cnp.intp_t right_node
+        cnp.intp_t left_node
+        cnp.intp_t new_node
+        cnp.intp_t i
+        cnp.intp_t j
+        cnp.intp_t dim
+        cnp.intp_t num_features
+
+        double current_node_core_distance
+        double right_value
+        double left_value
+        double core_value
+        double new_distance
 
     dim = raw_data.shape[0]
     num_features = raw_data.shape[1]
 
-    raw_data_view = (<np.double_t[:raw_data.shape[0], :raw_data.shape[1]:1]> (
-        <np.double_t *> raw_data.data))
-    raw_data_ptr = (<np.double_t *> &raw_data_view[0, 0])
+    raw_data_view = (<cnp.double_t[:raw_data.shape[0], :raw_data.shape[1]:1]> (
+        <cnp.double_t *> raw_data.data))
+    raw_data_ptr = (<cnp.double_t *> &raw_data_view[0, 0])
 
     result_arr = np.zeros((dim - 1, 3))
     in_tree_arr = np.zeros(dim, dtype=np.int8)
@@ -103,11 +105,11 @@
     current_distances_arr = np.infty * np.ones(dim)
     current_sources_arr = np.ones(dim)
 
-    result = (<np.double_t[:dim - 1, :3:1]> (<np.double_t *> result_arr.data))
-    in_tree = (<np.int8_t *> in_tree_arr.data)
-    current_distances = (<np.double_t *> current_distances_arr.data)
-    current_sources = (<np.double_t *> current_sources_arr.data)
-    current_core_distances = (<np.double_t *> core_distances.data)
+    result = (<cnp.double_t[:dim - 1, :3:1]> (<cnp.double_t *> result_arr.data))
+    in_tree = (<cnp.int8_t *> in_tree_arr.data)
+    current_distances = (<cnp.double_t *> current_distances_arr.data)
+    current_sources = (<cnp.double_t *> current_sources_arr.data)
+    current_core_distances = (<cnp.double_t *> core_distances.data)
 
     for i in range(1, dim):
 
@@ -175,21 +177,22 @@
 
 cdef class UnionFind (object):
 
-    cdef np.ndarray parent_arr
-    cdef np.ndarray size_arr
-    cdef np.intp_t next_label
-    cdef np.intp_t *parent
-    cdef np.intp_t *size
+    cdef:
+        cnp.ndarray parent_arr
+        cnp.ndarray size_arr
+        cnp.intp_t next_label
+        cnp.intp_t *parent
+        cnp.intp_t *size
 
     def __init__(self, N):
         self.parent_arr = -1 * np.ones(2 * N - 1, dtype=np.intp, order='C')
         self.next_label = N
         self.size_arr = np.hstack((np.ones(N, dtype=np.intp),
                                    np.zeros(N-1, dtype=np.intp)))
-        self.parent = (<np.intp_t *> self.parent_arr.data)
-        self.size = (<np.intp_t *> self.size_arr.data)
+        self.parent = (<cnp.intp_t *> self.parent_arr.data)
+        self.size = (<cnp.intp_t *> self.size_arr.data)
 
-    cdef void union(self, np.intp_t m, np.intp_t n):
+    cdef void union(self, cnp.intp_t m, cnp.intp_t n):
         self.size[self.next_label] = self.size[m] + self.size[n]
         self.parent[m] = self.next_label
         self.parent[n] = self.next_label
@@ -198,8 +201,9 @@
 
         return
 
-    cdef np.intp_t fast_find(self, np.intp_t n):
-        cdef np.intp_t p
+    @cython.wraparound(True)
+    cdef cnp.intp_t fast_find(self, cnp.intp_t n):
+        cdef cnp.intp_t p
         p = n
         while self.parent_arr[n] != -1:
             n = self.parent_arr[n]
@@ -208,25 +212,26 @@
             p, self.parent_arr[p] = self.parent_arr[p], n
         return n
 
+@cython.wraparound(True)
+cpdef cnp.ndarray[cnp.double_t, ndim=2] label(cnp.double_t[:,:] L):
 
-cpdef np.ndarray[np.double_t, ndim=2] label(np.ndarray[np.double_t, ndim=2] L):
+    cdef:
+        cnp.ndarray[cnp.double_t, ndim=2] result_arr
+        cnp.double_t[:, ::1] result
 
-    cdef np.ndarray[np.double_t, ndim=2] result_arr
-    cdef np.double_t[:, ::1] result
-
-    cdef np.intp_t N, a, aa, b, bb, index
-    cdef np.double_t delta
+        cnp.intp_t N, a, aa, b, bb, index
+        cnp.double_t delta
 
     result_arr = np.zeros((L.shape[0], L.shape[1] + 1))
-    result = (<np.double_t[:L.shape[0], :4:1]> (
-        <np.double_t *> result_arr.data))
+    result = (<cnp.double_t[:L.shape[0], :4:1]> (
+        <cnp.double_t *> result_arr.data))
     N = L.shape[0] + 1
     U = UnionFind(N)
 
     for index in range(L.shape[0]):
 
-        a = <np.intp_t> L[index, 0]
-        b = <np.intp_t> L[index, 1]
+        a = <cnp.intp_t> L[index, 0]
+        b = <cnp.intp_t> L[index, 1]
         delta = L[index, 2]
 
         aa, bb = U.fast_find(a), U.fast_find(b)
@@ -239,14 +244,3 @@
         U.union(aa, bb)
 
     return result_arr
-
-
-cpdef np.ndarray[np.double_t, ndim=2] single_linkage(distance_matrix):
-
-    cdef np.ndarray[np.double_t, ndim=2] hierarchy
-    cdef np.ndarray[np.double_t, ndim=2] for_labelling
-
-    hierarchy = mst_linkage_core(distance_matrix)
-    for_labelling = hierarchy[np.argsort(hierarchy.T[2]), :]
-
-    return label(for_labelling)
--- hdbscan/hdbscan/_hdbscan_reachability.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_reachability.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,20 +1,20 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
-# cython: initializedcheck=False
 # mutual reachability distance compiutations
 # Authors: Leland McInnes
 # License: 3-clause BSD
 
 import numpy as np
+
 cimport numpy as np
 
-from scipy.spatial.distance import pdist, squareform
-from scipy.sparse import lil_matrix as sparse_matrix
-from sklearn.neighbors import KDTree, BallTree
 import gc
 
+from scipy.sparse import lil_matrix as sparse_matrix
+from scipy.spatial.distance import pdist, squareform
 
-def mutual_reachability(distance_matrix, min_points=5, alpha=1.0):
+from sklearn.neighbors import BallTree, KDTree
+
+
+def mutual_reachability(distance_matrix, min_points=5, alpha=None):
     """Compute the weighted adjacency matrix of the mutual reachability
     graph of a distance matrix.
 
@@ -23,10 +23,14 @@
     distance_matrix : ndarray, shape (n_samples, n_samples)
         Array of distances between samples.
 
-    min_points : int, optional (default=5)
+    min_points : int, default=5
         The number of points in a neighbourhood for a point to be considered
         a core point.
 
+    alpha : float, default=None
+        A distance scaling parameter as used in robust single linkage. This
+        divides the distances when calculating mutual reachability.
+
     Returns
     -------
     mututal_reachability: ndarray, shape (n_samples, n_samples)
@@ -49,7 +53,7 @@
         core_distances = np.sort(distance_matrix,
                                  axis=0)[min_points]
 
-    if alpha != 1.0:
+    if alpha is not None:
         distance_matrix = distance_matrix / alpha
 
     stage1 = np.where(core_distances > distance_matrix,
@@ -97,117 +101,3 @@
             result[i, j] = max_dist
 
     return result.tocsr()
-
-
-def kdtree_mutual_reachability(X, distance_matrix, metric, p=2, min_points=5,
-                               alpha=1.0, **kwargs):
-    dim = distance_matrix.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    if metric == 'minkowski':
-        tree = KDTree(X, metric=metric, p=p)
-    else:
-        tree = KDTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    if alpha != 1.0:
-        distance_matrix = distance_matrix / alpha
-
-    stage1 = np.where(core_distances > distance_matrix,
-                      core_distances, distance_matrix)
-    result = np.where(core_distances > stage1.T,
-                      core_distances.T, stage1.T).T
-    return result
-
-
-def balltree_mutual_reachability(X, distance_matrix, metric, p=2, min_points=5,
-                                 alpha=1.0, **kwargs):
-    dim = distance_matrix.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    tree = BallTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    if alpha != 1.0:
-        distance_matrix = distance_matrix / alpha
-
-    stage1 = np.where(core_distances > distance_matrix,
-                      core_distances, distance_matrix)
-    result = np.where(core_distances > stage1.T,
-                      core_distances.T, stage1.T).T
-    return result
-
-
-cdef np.ndarray[np.double_t, ndim=1] mutual_reachability_from_pdist(
-        np.ndarray[np.double_t, ndim=1] core_distances,
-        np.ndarray[np.double_t, ndim=1] dists, np.intp_t dim):
-
-    cdef np.intp_t i
-    cdef np.intp_t j
-    cdef np.intp_t result_pos
-
-    result_pos = 0
-    for i in range(dim):
-        for j in range(i + 1, dim):
-            if core_distances[i] > core_distances[j]:
-                if core_distances[i] > dists[result_pos]:
-                    dists[result_pos] = core_distances[i]
-
-            else:
-                if core_distances[j] > dists[result_pos]:
-                    dists[result_pos] = core_distances[j]
-
-            result_pos += 1
-
-    return dists
-
-
-def kdtree_pdist_mutual_reachability(X,  metric, p=2, min_points=5, alpha=1.0,
-                                     **kwargs):
-
-    dim = X.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    if metric == 'minkowski':
-        tree = KDTree(X, metric=metric, p=p)
-    else:
-        tree = KDTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    del tree
-    gc.collect()
-
-    dists = pdist(X, metric=metric, p=p, **kwargs)
-
-    if alpha != 1.0:
-        dists /= alpha
-
-    dists = mutual_reachability_from_pdist(core_distances, dists, dim)
-
-    return dists
-
-
-def balltree_pdist_mutual_reachability(X, metric, p=2, min_points=5, alpha=1.0,
-                                       **kwargs):
-
-    dim = X.shape[0]
-    min_points = min(dim - 1, min_points)
-
-    tree = BallTree(X, metric=metric, **kwargs)
-
-    core_distances = tree.query(X, k=min_points)[0][:, -1]
-
-    del tree
-    gc.collect()
-
-    dists = pdist(X, metric=metric, p=p, **kwargs)
-
-    if alpha != 1.0:
-        dists /= alpha
-
-    dists = mutual_reachability_from_pdist(core_distances, dists, dim)
-
-    return dists
--- hdbscan/hdbscan/_hdbscan_tree.pyx	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/_tree.pyx	2022-08-29 17:03:02.000000000 -0400
@@ -1,13 +1,14 @@
-# cython: boundscheck=False
-# cython: nonecheck=False
-# cython: initializedcheck=False
 # Tree handling (condensing, finding stable clusters) for hdbscan
 # Authors: Leland McInnes
 # License: 3-clause BSD
 
 import numpy as np
+
 cimport numpy as np
 
+import cython
+
+
 cdef np.double_t INFTY = np.inf
 
 
@@ -295,7 +296,7 @@
             # Initialize
             current_parent = parent
             max_lambda = lambda_
-    
+
     deaths[current_parent] = max_lambda # value for last parent
 
     return deaths_arr
@@ -420,8 +421,7 @@
         set clusters,
         dict cluster_label_map,
         np.intp_t allow_single_cluster,
-        np.double_t cluster_selection_epsilon,
-        np.intp_t match_reference_implementation):
+        np.double_t cluster_selection_epsilon):
 
     cdef np.intp_t root_cluster
     cdef np.ndarray[np.intp_t, ndim=1] result_arr
@@ -470,15 +470,7 @@
             else:
                 result[n] = -1
         else:
-            if match_reference_implementation:
-                point_lambda = lambda_array[child_array == n][0]
-                cluster_lambda = lambda_array[child_array == cluster][0]
-                if point_lambda > cluster_lambda:
-                    result[n] = cluster_label_map[cluster]
-                else:
-                    result[n] = -1
-            else:
-                result[n] = cluster_label_map[cluster]
+            result[n] = cluster_label_map[cluster]
 
     return result_arr
 
@@ -527,85 +519,6 @@
     return result
 
 
-cpdef np.ndarray[np.double_t, ndim=1] outlier_scores(np.ndarray tree):
-    """Generate GLOSH outlier scores from a condensed tree.
-
-    Parameters
-    ----------
-    tree : numpy recarray
-        The condensed tree to generate GLOSH outlier scores from
-
-    Returns
-    -------
-    outlier_scores : ndarray (n_samples,)
-        Outlier scores for each sample point. The larger the score
-        the more outlying the point.
-    """
-
-    cdef np.ndarray[np.double_t, ndim=1] result
-    cdef np.ndarray[np.double_t, ndim=1] deaths
-    cdef np.ndarray[np.double_t, ndim=1] lambda_array
-    cdef np.ndarray[np.intp_t, ndim=1] child_array
-    cdef np.ndarray[np.intp_t, ndim=1] parent_array
-    cdef np.intp_t root_cluster
-    cdef np.intp_t point
-    cdef np.intp_t parent
-    cdef np.intp_t cluster
-    cdef np.double_t lambda_max
-
-    child_array = tree['child']
-    parent_array = tree['parent']
-    lambda_array = tree['lambda_val']
-
-    deaths = max_lambdas(tree)
-    root_cluster = parent_array.min()
-    result = np.zeros(root_cluster, dtype=np.double)
-
-    topological_sort_order = np.argsort(parent_array)
-    # topologically_sorted_tree = tree[topological_sort_order]
-
-    for n in topological_sort_order:
-        cluster = child_array[n]
-        if cluster < root_cluster:
-            break
-
-        parent = parent_array[n]
-        if deaths[cluster] > deaths[parent]:
-            deaths[parent] = deaths[cluster]
-
-    for n in range(tree.shape[0]):
-        point = child_array[n]
-        if point >= root_cluster:
-            continue
-
-        cluster = parent_array[n]
-        lambda_max = deaths[cluster]
-
-
-        if lambda_max == 0.0 or not np.isfinite(lambda_array[n]):
-            result[point] = 0.0
-        else:
-            result[point] = (lambda_max - lambda_array[n]) / lambda_max
-
-    return result
-
-
-cpdef np.ndarray get_stability_scores(np.ndarray labels, set clusters,
-                                      dict stability, np.double_t max_lambda):
-
-    cdef np.intp_t cluster_size
-    cdef np.intp_t n
-
-    result = np.empty(len(clusters), dtype=np.double)
-    for n, c in enumerate(sorted(list(clusters))):
-        cluster_size = np.sum(labels == n)
-        if np.isinf(max_lambda) or max_lambda == 0.0 or cluster_size == 0:
-            result[n] = 1.0
-        else:
-            result[n] = stability[c] / (cluster_size * max_lambda)
-
-    return result
-
 cpdef list recurse_leaf_dfs(np.ndarray cluster_tree, np.intp_t current_node):
     children = cluster_tree[cluster_tree['parent'] == current_node]['child']
     if len(children) == 0:
@@ -656,10 +569,10 @@
 
     return set(selected_clusters)
 
+@cython.wraparound(True)
 cpdef tuple get_clusters(np.ndarray tree, dict stability,
                          cluster_selection_method='eom',
                          allow_single_cluster=False,
-                         match_reference_implementation=False,
                          cluster_selection_epsilon=0.0,
                          max_cluster_size=0):
     """Given a tree and stability dict, produce the cluster labels
@@ -683,13 +596,9 @@
         Whether to allow a single cluster to be selected by the
         Excess of Mass algorithm.
 
-    match_reference_implementation : boolean, optional (default False)
-        Whether to match the reference implementation in how to handle
-        certain edge cases.
-
     cluster_selection_epsilon: float, optional (default 0.0)
         A distance threshold for cluster splits.
-        
+
     max_cluster_size: int, optional (default 0)
         The maximum size for clusters located by the EOM clusterer. Can
         be overridden by the cluster_selection_epsilon parameter in
@@ -798,9 +707,7 @@
     reverse_cluster_map = {n: c for c, n in cluster_map.items()}
 
     labels = do_labelling(tree, clusters, cluster_map,
-                          allow_single_cluster, cluster_selection_epsilon,
-                          match_reference_implementation)
+                          allow_single_cluster, cluster_selection_epsilon)
     probs = get_probabilities(tree, reverse_cluster_map, labels)
-    stabilities = get_stability_scores(labels, clusters, stability, max_lambda)
 
-    return (labels, probs, stabilities)
+    return (labels, probs)
--- hdbscan/hdbscan/hdbscan_.py	2022-08-29 17:02:16.000000000 -0400
+++ scikit-learn/sklearn/cluster/_hdbscan/hdbscan.py	2022-08-29 17:03:02.000000000 -0400
@@ -1,59 +1,69 @@
-# -*- coding: utf-8 -*-
 """
 HDBSCAN: Hierarchical Density-Based Spatial Clustering
          of Applications with Noise
 """
+# Author: Leland McInnes <leland.mcinnes@gmail.com>
+#         Steve Astels <sastels@gmail.com>
+#         John Healy <jchealy@gmail.com>
+#
+# License: BSD 3 clause
+
+from numbers import Integral, Real
+from pathlib import Path
+from warnings import warn
 
 import numpy as np
+from joblib import Memory
+from scipy.sparse import csgraph, issparse
 
 from sklearn.base import BaseEstimator, ClusterMixin
 from sklearn.metrics import pairwise_distances
-from scipy.sparse import issparse
-from sklearn.neighbors import KDTree, BallTree
-from joblib import Memory
-from warnings import warn
-from sklearn.utils import check_array
-from joblib.parallel import cpu_count
-
-from scipy.sparse import csgraph
-
-from ._hdbscan_linkage import (
-    single_linkage,
-    mst_linkage_core,
-    mst_linkage_core_vector,
-    label,
-)
-from ._hdbscan_tree import (
-    condense_tree,
+from sklearn.metrics._dist_metrics import DistanceMetric
+from sklearn.neighbors import BallTree, KDTree, NearestNeighbors
+from sklearn.utils import check_array, gen_batches, get_chunk_n_rows
+from sklearn.utils._param_validation import Interval, StrOptions, validate_params
+
+from ._linkage import label, mst_linkage_core, mst_linkage_core_vector
+from ._reachability import mutual_reachability, sparse_mutual_reachability
+from ._tree import (
     compute_stability,
+    condense_tree,
     get_clusters,
-    outlier_scores,
+    labelling_at_cut,
 )
-from ._hdbscan_reachability import mutual_reachability, sparse_mutual_reachability
-
-from ._hdbscan_boruvka import KDTreeBoruvkaAlgorithm, BallTreeBoruvkaAlgorithm
-from .dist_metrics import DistanceMetric
 
-from .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree
-from .prediction import PredictionData
-
-FAST_METRICS = KDTree.valid_metrics + BallTree.valid_metrics + ["cosine", "arccos"]
-
-# Author: Leland McInnes <leland.mcinnes@gmail.com>
-#         Steve Astels <sastels@gmail.com>
-#         John Healy <jchealy@gmail.com>
-#
-# License: BSD 3 clause
-from numpy import isclose
+FAST_METRICS = KDTree.valid_metrics + BallTree.valid_metrics
+_PARAM_CONSTRAINTS = {
+    "min_cluster_size": [Interval(Integral, left=2, right=None, closed="left")],
+    "min_samples": [Interval(Integral, left=1, right=None, closed="left"), None],
+    "cluster_selection_epsilon": [Interval(Real, left=0, right=None, closed="left")],
+    "max_cluster_size": [Interval(Integral, left=0, right=None, closed="left")],
+    "metric": [StrOptions(set(FAST_METRICS + ["precomputed"])), callable],
+    "alpha": [Interval(Real, left=0, right=None, closed="neither")],
+    "algorithm": [
+        StrOptions(
+            {
+                "auto",
+                "brute",
+                "kdtree",
+                "balltree",
+            }
+        )
+    ],
+    "leaf_size": [Interval(Integral, left=1, right=None, closed="left")],
+    "memory": [str, None, Path],
+    "n_jobs": [int],
+    "cluster_selection_method": [StrOptions({"eom", "leaf"})],
+    "allow_single_cluster": ["boolean"],
+    "metric_params": [dict, None],
+}
 
 
 def _tree_to_labels(
-    X,
     single_linkage_tree,
     min_cluster_size=10,
     cluster_selection_method="eom",
     allow_single_cluster=False,
-    match_reference_implementation=False,
     cluster_selection_epsilon=0.0,
     max_cluster_size=0,
 ):
@@ -62,54 +72,48 @@
     """
     condensed_tree = condense_tree(single_linkage_tree, min_cluster_size)
     stability_dict = compute_stability(condensed_tree)
-    labels, probabilities, stabilities = get_clusters(
+    labels, probabilities = get_clusters(
         condensed_tree,
         stability_dict,
         cluster_selection_method,
         allow_single_cluster,
-        match_reference_implementation,
         cluster_selection_epsilon,
         max_cluster_size,
     )
 
-    return (labels, probabilities, stabilities, condensed_tree, single_linkage_tree)
+    return (labels, probabilities, single_linkage_tree)
 
 
-def _hdbscan_generic(
+def _process_mst(min_spanning_tree):
+    # Sort edges of the min_spanning_tree by weight
+    row_order = np.argsort(min_spanning_tree.T[2])
+    min_spanning_tree = min_spanning_tree[row_order, :]
+    # Convert edge list into standard hierarchical clustering format
+    return label(min_spanning_tree)
+
+
+def _hdbscan_brute(
     X,
     min_samples=5,
     alpha=1.0,
-    metric="minkowski",
-    p=2,
-    leaf_size=None,
-    gen_min_span_tree=False,
-    **kwargs
+    metric="euclidean",
+    **metric_params,
 ):
-    if metric == "minkowski":
-        distance_matrix = pairwise_distances(X, metric=metric, p=p)
-    elif metric == "arccos":
-        distance_matrix = pairwise_distances(X, metric="cosine", **kwargs)
-    elif metric == "precomputed":
+    if metric == "precomputed":
         # Treating this case explicitly, instead of letting
-        #   sklearn.metrics.pairwise_distances handle it,
-        #   enables the usage of numpy.inf in the distance
-        #   matrix to indicate missing distance information.
-        # TODO: Check if copying is necessary
-        distance_matrix = X.copy()
+        # sklearn.metrics.pairwise_distances handle it,
+        # enables the usage of numpy.inf in the distance
+        # matrix to indicate missing distance information.
+        distance_matrix = X
     else:
-        distance_matrix = pairwise_distances(X, metric=metric, **kwargs)
+        distance_matrix = pairwise_distances(X, metric=metric, **metric_params)
 
     if issparse(distance_matrix):
-        # raise TypeError('Sparse distance matrices not yet supported')
         return _hdbscan_sparse_distance_matrix(
             distance_matrix,
             min_samples,
             alpha,
-            metric,
-            p,
-            leaf_size,
-            gen_min_span_tree,
-            **kwargs
+            **metric_params,
         )
 
     mutual_reachability_ = mutual_reachability(distance_matrix, min_samples, alpha)
@@ -126,59 +130,22 @@
             UserWarning,
         )
 
-    # mst_linkage_core does not generate a full minimal spanning tree
-    # If a tree is required then we must build the edges from the information
-    # returned by mst_linkage_core (i.e. just the order of points to be merged)
-    if gen_min_span_tree:
-        result_min_span_tree = min_spanning_tree.copy()
-        for index, row in enumerate(result_min_span_tree[1:], 1):
-            candidates = np.where(isclose(mutual_reachability_[int(row[1])], row[2]))[0]
-            candidates = np.intersect1d(
-                candidates, min_spanning_tree[:index, :2].astype(int)
-            )
-            candidates = candidates[candidates != row[1]]
-            assert len(candidates) > 0
-            row[0] = candidates[0]
-    else:
-        result_min_span_tree = None
-
-    # Sort edges of the min_spanning_tree by weight
-    min_spanning_tree = min_spanning_tree[np.argsort(min_spanning_tree.T[2]), :]
-
-    # Convert edge list into standard hierarchical clustering format
-    single_linkage_tree = label(min_spanning_tree)
-
-    return single_linkage_tree, result_min_span_tree
+    return _process_mst(min_spanning_tree)
 
 
 def _hdbscan_sparse_distance_matrix(
     X,
     min_samples=5,
     alpha=1.0,
-    metric="minkowski",
-    p=2,
-    leaf_size=40,
-    gen_min_span_tree=False,
-    **kwargs
+    **metric_params,
 ):
     assert issparse(X)
-    # Check for connected component on X
-    if csgraph.connected_components(X, directed=False, return_labels=False) > 1:
-        raise ValueError(
-            "Sparse distance matrix has multiple connected "
-            "components!\nThat is, there exist groups of points "
-            "that are completely disjoint -- there are no distance "
-            "relations connecting them\n"
-            "Run hdbscan on each component."
-        )
-
-    lil_matrix = X.tolil()
 
     # Compute sparse mutual reachability graph
     # if max_dist > 0, max distance to use when the reachability is infinite
-    max_dist = kwargs.get("max_dist", 0.0)
+    max_dist = metric_params.get("max_dist", 0.0)
     mutual_reachability_ = sparse_mutual_reachability(
-        lil_matrix, min_points=min_samples, max_dist=max_dist, alpha=alpha
+        X.tolil(), min_points=min_samples, max_dist=max_dist, alpha=alpha
     )
     # Check connected component on mutual reachability
     # If more than one component, it means that even if the distance matrix X
@@ -190,12 +157,10 @@
         > 1
     ):
         raise ValueError(
-            (
-                "There exists points with less than %s neighbors. "
-                "Ensure your distance matrix has non zeros values for "
-                "at least `min_sample`=%s neighbors for each points (i.e. K-nn graph), "
-                "or specify a `max_dist` to use when distances are missing."
-            )
+            "There exists points with less than %s neighbors. "
+            "Ensure your distance matrix has non zeros values for "
+            "at least `min_sample`=%s neighbors for each points (i.e. K-nn graph), "
+            "or specify a `max_dist` to use when distances are missing."
             % (min_samples, min_samples)
         )
 
@@ -213,228 +178,49 @@
     # Convert edge list into standard hierarchical clustering format
     single_linkage_tree = label(min_spanning_tree)
 
-    if gen_min_span_tree:
-        return single_linkage_tree, min_spanning_tree
-    else:
-        return single_linkage_tree, None
+    return single_linkage_tree
 
 
-def _hdbscan_prims_kdtree(
+def _hdbscan_prims(
     X,
+    algo,
     min_samples=5,
     alpha=1.0,
-    metric="minkowski",
-    p=2,
+    metric="euclidean",
     leaf_size=40,
-    gen_min_span_tree=False,
-    **kwargs
+    n_jobs=4,
+    **metric_params,
 ):
-    if X.dtype != np.float64:
-        X = X.astype(np.float64)
-
     # The Cython routines used require contiguous arrays
     if not X.flags["C_CONTIGUOUS"]:
-        X = np.array(X, dtype=np.double, order="C")
-
-    tree = KDTree(X, metric=metric, leaf_size=leaf_size, **kwargs)
-
-    # TO DO: Deal with p for minkowski appropriately
-    dist_metric = DistanceMetric.get_metric(metric, **kwargs)
+        X = np.array(X, order="C")
 
     # Get distance to kth nearest neighbour
-    core_distances = tree.query(
-        X, k=min_samples + 1, dualtree=True, breadth_first=True
-    )[0][:, -1].copy(order="C")
-
-    # Mutual reachability distance is implicit in mst_linkage_core_vector
-    min_spanning_tree = mst_linkage_core_vector(X, core_distances, dist_metric, alpha)
-
-    # Sort edges of the min_spanning_tree by weight
-    min_spanning_tree = min_spanning_tree[np.argsort(min_spanning_tree.T[2]), :]
-
-    # Convert edge list into standard hierarchical clustering format
-    single_linkage_tree = label(min_spanning_tree)
-
-    if gen_min_span_tree:
-        return single_linkage_tree, min_spanning_tree
-    else:
-        return single_linkage_tree, None
-
-
-def _hdbscan_prims_balltree(
-    X,
-    min_samples=5,
-    alpha=1.0,
-    metric="minkowski",
-    p=2,
-    leaf_size=40,
-    gen_min_span_tree=False,
-    **kwargs
-):
-    if X.dtype != np.float64:
-        X = X.astype(np.float64)
-
-    # The Cython routines used require contiguous arrays
-    if not X.flags["C_CONTIGUOUS"]:
-        X = np.array(X, dtype=np.double, order="C")
-
-    tree = BallTree(X, metric=metric, leaf_size=leaf_size, **kwargs)
+    nbrs = NearestNeighbors(
+        n_neighbors=min_samples,
+        algorithm=algo,
+        leaf_size=leaf_size,
+        metric=metric,
+        metric_params=metric_params,
+        n_jobs=n_jobs,
+        p=None,
+    ).fit(X)
 
-    dist_metric = DistanceMetric.get_metric(metric, **kwargs)
+    n_samples = X.shape[0]
+    core_distances = np.empty(n_samples)
+    core_distances.fill(np.nan)
+
+    chunk_n_rows = get_chunk_n_rows(row_bytes=16 * min_samples, max_n_rows=n_samples)
+    slices = gen_batches(n_samples, chunk_n_rows)
+    for sl in slices:
+        core_distances[sl] = nbrs.kneighbors(X[sl], min_samples)[0][:, -1]
 
-    # Get distance to kth nearest neighbour
-    core_distances = tree.query(
-        X, k=min_samples + 1, dualtree=True, breadth_first=True
-    )[0][:, -1].copy(order="C")
+    dist_metric = DistanceMetric.get_metric(metric, **metric_params)
 
     # Mutual reachability distance is implicit in mst_linkage_core_vector
     min_spanning_tree = mst_linkage_core_vector(X, core_distances, dist_metric, alpha)
-    # Sort edges of the min_spanning_tree by weight
-    min_spanning_tree = min_spanning_tree[np.argsort(min_spanning_tree.T[2]), :]
-    # Convert edge list into standard hierarchical clustering format
-    single_linkage_tree = label(min_spanning_tree)
-
-    if gen_min_span_tree:
-        return single_linkage_tree, min_spanning_tree
-    else:
-        return single_linkage_tree, None
-
-
-def _hdbscan_boruvka_kdtree(
-    X,
-    min_samples=5,
-    alpha=1.0,
-    metric="minkowski",
-    p=2,
-    leaf_size=40,
-    approx_min_span_tree=True,
-    gen_min_span_tree=False,
-    core_dist_n_jobs=4,
-    **kwargs
-):
-    if leaf_size < 3:
-        leaf_size = 3
-
-    if core_dist_n_jobs < 1:
-        core_dist_n_jobs = max(cpu_count() + 1 + core_dist_n_jobs, 1)
-
-    if X.dtype != np.float64:
-        X = X.astype(np.float64)
-
-    tree = KDTree(X, metric=metric, leaf_size=leaf_size, **kwargs)
-    alg = KDTreeBoruvkaAlgorithm(
-        tree,
-        min_samples,
-        metric=metric,
-        leaf_size=leaf_size // 3,
-        approx_min_span_tree=approx_min_span_tree,
-        n_jobs=core_dist_n_jobs,
-        **kwargs
-    )
-    min_spanning_tree = alg.spanning_tree()
-    # Sort edges of the min_spanning_tree by weight
-    row_order = np.argsort(min_spanning_tree.T[2])
-    min_spanning_tree = min_spanning_tree[row_order, :]
-    # Convert edge list into standard hierarchical clustering format
-    single_linkage_tree = label(min_spanning_tree)
-
-    if gen_min_span_tree:
-        return single_linkage_tree, min_spanning_tree
-    else:
-        return single_linkage_tree, None
-
-
-def _hdbscan_boruvka_balltree(
-    X,
-    min_samples=5,
-    alpha=1.0,
-    metric="minkowski",
-    p=2,
-    leaf_size=40,
-    approx_min_span_tree=True,
-    gen_min_span_tree=False,
-    core_dist_n_jobs=4,
-    **kwargs
-):
-    if leaf_size < 3:
-        leaf_size = 3
-
-    if core_dist_n_jobs < 1:
-        core_dist_n_jobs = max(cpu_count() + 1 + core_dist_n_jobs, 1)
-
-    if X.dtype != np.float64:
-        X = X.astype(np.float64)
-
-    tree = BallTree(X, metric=metric, leaf_size=leaf_size, **kwargs)
-    alg = BallTreeBoruvkaAlgorithm(
-        tree,
-        min_samples,
-        metric=metric,
-        leaf_size=leaf_size // 3,
-        approx_min_span_tree=approx_min_span_tree,
-        n_jobs=core_dist_n_jobs,
-        **kwargs
-    )
-    min_spanning_tree = alg.spanning_tree()
-    # Sort edges of the min_spanning_tree by weight
-    min_spanning_tree = min_spanning_tree[np.argsort(min_spanning_tree.T[2]), :]
-    # Convert edge list into standard hierarchical clustering format
-    single_linkage_tree = label(min_spanning_tree)
-
-    if gen_min_span_tree:
-        return single_linkage_tree, min_spanning_tree
-    else:
-        return single_linkage_tree, None
-
-
-def check_precomputed_distance_matrix(X):
-    """Perform check_array(X) after removing infinite values (numpy.inf) from the given distance matrix."""
-    tmp = X.copy()
-    tmp[np.isinf(tmp)] = 1
-    check_array(tmp)
-
-
-def remap_condensed_tree(tree, internal_to_raw, outliers):
-    """
-    Takes an internal condensed_tree structure and adds back in a set of points
-    that were initially detected as non-finite and returns that new tree.
-    These points will all be split off from the maximal node at lambda zero and
-    considered noise points.
-
-    Parameters
-    ----------
-    tree: condensed_tree
-    internal_to_raw: dict
-        a mapping from internal integer index to the raw integer index
-    finite_index: ndarray
-        Boolean array of which entries in the raw data were finite
-    """
-    finite_count = len(internal_to_raw)
 
-    outlier_count = len(outliers)
-    for i, (parent, child, lambda_val, child_size) in enumerate(tree):
-        if child < finite_count:
-            child = internal_to_raw[child]
-        else:
-            child = child + outlier_count
-        tree[i] = (parent + outlier_count, child, lambda_val, child_size)
-
-    outlier_list = []
-    root = tree[0][0]  # Should I check to be sure this is the minimal lambda?
-    for outlier in outliers:
-        outlier_list.append((root, outlier, 0, 1))
-
-    outlier_tree = np.array(
-        outlier_list,
-        dtype=[
-            ("parent", np.intp),
-            ("child", np.intp),
-            ("lambda_val", float),
-            ("child_size", np.intp),
-        ],
-    )
-    tree = np.append(outlier_tree, tree)
-    return tree
+    return _process_mst(min_spanning_tree)
 
 
 def remap_single_linkage_tree(tree, internal_to_raw, outliers):
@@ -476,16 +262,11 @@
     return tree
 
 
-def is_finite(matrix):
-    """Returns true only if all the values of a ndarray or sparse matrix are finite"""
-    if issparse(matrix):
-        return np.alltrue(np.isfinite(matrix.tocoo().data))
-    else:
-        return np.alltrue(np.isfinite(matrix))
-
-
 def get_finite_row_indices(matrix):
-    """Returns the indices of the purely finite rows of a sparse matrix or dense ndarray"""
+    """
+    Returns the indices of the purely finite rows of a
+    sparse matrix or dense ndarray
+    """
     if issparse(matrix):
         row_indices = np.array(
             [i for i, row in enumerate(matrix.tolil().data) if np.all(np.isfinite(row))]
@@ -495,6 +276,12 @@
     return row_indices
 
 
+@validate_params(
+    {
+        **_PARAM_CONSTRAINTS,
+        "X": ["array-like", "sparse matrix"],
+    }
+)
 def hdbscan(
     X,
     min_cluster_size=5,
@@ -502,18 +289,14 @@
     alpha=1.0,
     cluster_selection_epsilon=0.0,
     max_cluster_size=0,
-    metric="minkowski",
-    p=2,
+    metric="euclidean",
     leaf_size=40,
-    algorithm="best",
-    memory=Memory(cachedir=None, verbose=0),
-    approx_min_span_tree=True,
-    gen_min_span_tree=False,
-    core_dist_n_jobs=4,
+    algorithm="auto",
+    memory=None,
+    n_jobs=4,
     cluster_selection_method="eom",
     allow_single_cluster=False,
-    match_reference_implementation=False,
-    **kwargs
+    metric_params=None,
 ):
     """Perform HDBSCAN clustering from a vector array or distance matrix.
 
@@ -522,111 +305,89 @@
     X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
             array of shape (n_samples, n_samples)
         A feature array, or array of distances between samples if
-        ``metric='precomputed'``.
+        `metric='precomputed'`.
 
-    min_cluster_size : int, optional (default=5)
+    min_cluster_size : int, default=5
         The minimum number of samples in a group for that group to be
         considered a cluster; groupings smaller than this size will be left
         as noise.
 
-    min_samples : int, optional (default=None)
+    min_samples : int, default=None
         The number of samples in a neighborhood for a point
         to be considered as a core point. This includes the point itself.
-        defaults to the min_cluster_size.
+        defaults to the `min_cluster_size`.
 
-    cluster_selection_epsilon: float, optional (default=0.0)
-        A distance threshold. Clusters below this value will be merged.
-        See [3]_ for more information. Note that this should not be used
-        if we want to predict the cluster labels for new points in future
-        (e.g. using approximate_predict), as the approximate_predict function
-        is not aware of this argument.
-
-    alpha : float, optional (default=1.0)
+    alpha : float, default=1.0
         A distance scaling parameter as used in robust single linkage.
         See [2]_ for more information.
 
-    max_cluster_size : int, optional (default=0)
-        A limit to the size of clusters returned by the eom algorithm.
-        Has no effect when using leaf clustering (where clusters are
-        usually small regardless) and can also be overridden in rare
-        cases by a high value for cluster_selection_epsilon. Note that
-        this should not be used if we want to predict the cluster labels
-        for new points in future (e.g. using approximate_predict), as
-        the approximate_predict function is not aware of this argument.
+    cluster_selection_epsilon : float, default=0.0
+        A distance threshold. Clusters below this value will be merged.
+        See [3]_ for more information.
+
+    max_cluster_size : int, default=0
+        A limit to the size of clusters returned by the `eom` cluster selection
+        algorithm. Has no effect if `cluster_selection_method=leaf`. Can be
+        overridden in rare cases by a high value for
+        `cluster_selection_epsilon`.
 
-    metric : string or callable, optional (default='minkowski')
+    metric : str or callable, default='minkowski'
         The metric to use when calculating distance between instances in a
-        feature array. If metric is a string or callable, it must be one of
-        the options allowed by metrics.pairwise.pairwise_distances for its
-        metric parameter.
-        If metric is "precomputed", X is assumed to be a distance matrix and
-        must be square.
-
-    p : int, optional (default=2)
-        p value to use if using the minkowski metric.
-
-    leaf_size : int, optional (default=40)
-        Leaf size for trees responsible for fast nearest
-        neighbour queries.
+        feature array.
+
+        - If metric is a string or callable, it must be one of
+          the options allowed by :func:`metrics.pairwise.pairwise_distances`
+          for its metric parameter.
+
+        - If metric is "precomputed", `X` is assumed to be a distance matrix and
+          must be square.
+
+    leaf_size : int, default=40
+        Leaf size for trees responsible for fast nearest neighbour queries. A
+        large dataset size and small leaf_size may induce excessive memory
+        usage. If you are running out of memory consider increasing the
+        `leaf_size` parameter.
 
-    algorithm : string, optional (default='best')
+    algorithm : str, default='auto'
         Exactly which algorithm to use; hdbscan has variants specialised
         for different characteristics of the data. By default this is set
-        to ``best`` which chooses the "best" algorithm given the nature of
-        the data. You can force other options if you believe you know
-        better. Options are:
-            * ``best``
-            * ``generic``
-            * ``prims_kdtree``
-            * ``prims_balltree``
-            * ``boruvka_kdtree``
-            * ``boruvka_balltree``
+        to `'auto'` which attempts to use a `KDTree` tree if possible,
+        otherwise it uses a `BallTree` tree.
+
+        If `X` is sparse or `metric` is invalid for both `KDTree` and
+        `BallTree`, then it resolves to use the `brute` algorithm.
+
+        Available algorithms:
+        - `'brute'`
+        - `'kdtree'`
+        - `'balltree'`
 
-    memory : instance of joblib.Memory or string, optional
+    memory : str, default=None
         Used to cache the output of the computation of the tree.
         By default, no caching is done. If a string is given, it is the
         path to the caching directory.
 
-    approx_min_span_tree : bool, optional (default=True)
-        Whether to accept an only approximate minimum spanning tree.
-        For some algorithms this can provide a significant speedup, but
-        the resulting clustering may be of marginally lower quality.
-        If you are willing to sacrifice speed for correctness you may want
-        to explore this; in general this should be left at the default True.
-
-    gen_min_span_tree : bool, optional (default=False)
-        Whether to generate the minimum spanning tree for later analysis.
-
-    core_dist_n_jobs : int, optional (default=4)
+    n_jobs : int, default=4
         Number of parallel jobs to run in core distance computations (if
-        supported by the specific algorithm). For ``core_dist_n_jobs``
-        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.
+        supported by the specific algorithm). For `n_jobs<0`,
+        `(n_cpus + n_jobs + 1)` are used.
 
-    cluster_selection_method : string, optional (default='eom')
+    cluster_selection_method : str, default='eom'
         The method used to select clusters from the condensed tree. The
         standard approach for HDBSCAN* is to use an Excess of Mass algorithm
         to find the most persistent clusters. Alternatively you can instead
         select the clusters at the leaves of the tree -- this provides the
         most fine grained and homogeneous clusters. Options are:
-            * ``eom``
-            * ``leaf``
+        - `eom`
+        - `leaf`
 
-    allow_single_cluster : bool, optional (default=False)
-        By default HDBSCAN* will not produce a single cluster, setting this
-        to t=True will override this and allow single cluster results in
-        the case that you feel this is a valid result for your dataset.
-        (default False)
+    allow_single_cluster : bool, default=False
+        By default HDBSCAN* will not produce a single cluster. Setting this to
+        `True` will allow single cluster results in the case that you feel this
+        is a valid result for your dataset.
 
-    match_reference_implementation : bool, optional (default=False)
-        There exist some interpretational differences between this
-        HDBSCAN* implementation and the original authors reference
-        implementation in Java. This can result in very minor differences
-        in clustering results. Setting this flag to True will, at a some
-        performance cost, ensure that the clustering results match the
-        reference implementation.
-
-    **kwargs : optional
-        Arguments passed to the distance metric
+    metric_params : dict, default=None
+        Arguments passed to the distance metric.
 
     Returns
     -------
@@ -637,25 +398,11 @@
         Cluster membership strengths for each point. Noisy samples are assigned
         0.
 
-    cluster_persistence : array, shape  (n_clusters, )
-        A score of how persistent each cluster is. A score of 1.0 represents
-        a perfectly stable cluster that persists over all distance scales,
-        while a score of 0.0 represents a perfectly ephemeral cluster. These
-        scores can be guage the relative coherence of the clusters output
-        by the algorithm.
-
-    condensed_tree : record array
-        The condensed cluster hierarchy used to generate clusters.
-
     single_linkage_tree : ndarray, shape (n_samples - 1, 4)
         The single linkage tree produced during clustering in scipy
         hierarchical clustering format
         (see http://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).
 
-    min_spanning_tree : ndarray, shape (n_samples - 1, 3)
-        The minimum spanning as an edgelist. If gen_min_span_tree was False
-        this will be None.
-
     References
     ----------
 
@@ -668,220 +415,90 @@
        cluster tree. In Advances in Neural Information Processing Systems
        (pp. 343-351).
 
-    .. [3] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical 
-	   Density-based Cluster Selection. arxiv preprint 1911.02282.
+    .. [3] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical
+       Density-based Cluster Selection. arxiv preprint 1911.02282.
     """
     if min_samples is None:
         min_samples = min_cluster_size
 
-    if not np.issubdtype(type(min_samples), np.integer) or \
-       not np.issubdtype(type(min_cluster_size), np.integer):
-        raise ValueError("Min samples and min cluster size must be integers!")
+    # Checks input and converts to an nd-array where possible
+    if metric != "precomputed" or issparse(X):
+        X = check_array(
+            X, accept_sparse="csr", force_all_finite=False, dtype=np.float64
+        )
+    elif isinstance(X, np.ndarray):
+        # Only non-sparse, precomputed distance matrices are handled here
+        # and thereby allowed to contain numpy.inf for missing distances
 
-    if min_samples <= 0 or min_cluster_size <= 0:
+        # Perform check_array(X) after removing infinite values (numpy.inf)
+        # from the given distance matrix.
+        tmp = X.copy()
+        tmp[np.isinf(tmp)] = 1
+        check_array(tmp)
+
+    memory = Memory(location=memory, verbose=0)
+
+    metric_params = metric_params or {}
+    func = None
+    kwargs = dict(
+        X=X,
+        algo="kd_tree",
+        min_samples=min_samples,
+        alpha=alpha,
+        metric=metric,
+        leaf_size=leaf_size,
+        n_jobs=n_jobs,
+        **metric_params,
+    )
+    if "kdtree" in algorithm and metric not in KDTree.valid_metrics:
         raise ValueError(
-            "Min samples and Min cluster size must be positive" " integers"
+            f"{metric} is not a valid metric for a KDTree-based algorithm. Please"
+            " select a different metric."
         )
-
-    if min_cluster_size == 1:
-        raise ValueError("Min cluster size must be greater than one")
-
-    if np.issubdtype(type(cluster_selection_epsilon), np.integer):
-        cluster_selection_epsilon = float(cluster_selection_epsilon)
-
-    if type(cluster_selection_epsilon) is not float or cluster_selection_epsilon < 0.0:
-        raise ValueError("Epsilon must be a float value greater than or equal to 0!")
-
-    if not isinstance(alpha, float) or alpha <= 0.0:
-        raise ValueError("Alpha must be a positive float value greater than" " 0!")
-
-    if leaf_size < 1:
-        raise ValueError("Leaf size must be greater than 0!")
-
-    if metric == "minkowski":
-        if p is None:
-            raise TypeError("Minkowski metric given but no p value supplied!")
-        if p < 0:
-            raise ValueError(
-                "Minkowski metric with negative p value is not" " defined!"
-            )
-
-    if match_reference_implementation:
-        min_samples = min_samples - 1
-        min_cluster_size = min_cluster_size + 1
-        approx_min_span_tree = False
-
-    if cluster_selection_method not in ("eom", "leaf"):
+    elif "balltree" in algorithm and metric not in BallTree.valid_metrics:
         raise ValueError(
-            "Invalid Cluster Selection Method: %s\n" 'Should be one of: "eom", "leaf"\n'
+            f"{metric} is not a valid metric for a BallTree-based algorithm. Please"
+            " select a different metric."
         )
 
-    # Checks input and converts to an nd-array where possible
-    if metric != "precomputed" or issparse(X):
-        X = check_array(X, accept_sparse="csr", force_all_finite=False)
+    if algorithm != "auto":
+        if metric != "precomputed" and issparse(X) and algorithm != "brute":
+            raise ValueError("Sparse data matrices only support algorithm `brute`.")
+
+        if algorithm == "brute":
+            func = _hdbscan_brute
+            for key in ("algo", "leaf_size", "n_jobs"):
+                kwargs.pop(key, None)
+        elif algorithm == "kdtree":
+            func = _hdbscan_prims
+        elif algorithm == "balltree":
+            func = _hdbscan_prims
+            kwargs["algo"] = "ball_tree"
     else:
-        # Only non-sparse, precomputed distance matrices are handled here
-        #   and thereby allowed to contain numpy.inf for missing distances
-        check_precomputed_distance_matrix(X)
-
-    # Python 2 and 3 compliant string_type checking
-    if isinstance(memory, str):
-        memory = Memory(cachedir=memory, verbose=0)
-
-    size = X.shape[0]
-    min_samples = min(size - 1, min_samples)
-    if min_samples == 0:
-        min_samples = 1
-
-    if algorithm != "best":
-        if metric != "precomputed" and issparse(X) and algorithm != "generic":
-            raise ValueError("Sparse data matrices only support algorithm 'generic'.")
-
-        if algorithm == "generic":
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_generic
-            )(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)
-        elif algorithm == "prims_kdtree":
-            if metric not in KDTree.valid_metrics:
-                raise ValueError("Cannot use Prim's with KDTree for this" " metric!")
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_prims_kdtree
-            )(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)
-        elif algorithm == "prims_balltree":
-            if metric not in BallTree.valid_metrics:
-                raise ValueError("Cannot use Prim's with BallTree for this" " metric!")
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_prims_balltree
-            )(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)
-        elif algorithm == "boruvka_kdtree":
-            if metric not in BallTree.valid_metrics:
-                raise ValueError("Cannot use Boruvka with KDTree for this" " metric!")
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_boruvka_kdtree
-            )(
-                X,
-                min_samples,
-                alpha,
-                metric,
-                p,
-                leaf_size,
-                approx_min_span_tree,
-                gen_min_span_tree,
-                core_dist_n_jobs,
-                **kwargs
-            )
-        elif algorithm == "boruvka_balltree":
-            if metric not in BallTree.valid_metrics:
-                raise ValueError("Cannot use Boruvka with BallTree for this" " metric!")
-            if (X.shape[0] // leaf_size) > 16000:
-                warn(
-                    "A large dataset size and small leaf_size may induce excessive "
-                    "memory usage. If you are running out of memory consider "
-                    "increasing the ``leaf_size`` parameter."
-                )
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_boruvka_balltree
-            )(
-                X,
-                min_samples,
-                alpha,
-                metric,
-                p,
-                leaf_size,
-                approx_min_span_tree,
-                gen_min_span_tree,
-                core_dist_n_jobs,
-                **kwargs
-            )
-        else:
-            raise TypeError("Unknown algorithm type %s specified" % algorithm)
-    else:
-
         if issparse(X) or metric not in FAST_METRICS:
             # We can't do much with sparse matrices ...
-            (single_linkage_tree, result_min_span_tree) = memory.cache(
-                _hdbscan_generic
-            )(X, min_samples, alpha, metric, p, leaf_size, gen_min_span_tree, **kwargs)
+            func = _hdbscan_brute
+            for key in ("algo", "leaf_size", "n_jobs"):
+                kwargs.pop(key, None)
         elif metric in KDTree.valid_metrics:
-            # TO DO: Need heuristic to decide when to go to boruvka;
-            # still debugging for now
-            if X.shape[1] > 60:
-                (single_linkage_tree, result_min_span_tree) = memory.cache(
-                    _hdbscan_prims_kdtree
-                )(
-                    X,
-                    min_samples,
-                    alpha,
-                    metric,
-                    p,
-                    leaf_size,
-                    gen_min_span_tree,
-                    **kwargs
-                )
-            else:
-                (single_linkage_tree, result_min_span_tree) = memory.cache(
-                    _hdbscan_boruvka_kdtree
-                )(
-                    X,
-                    min_samples,
-                    alpha,
-                    metric,
-                    p,
-                    leaf_size,
-                    approx_min_span_tree,
-                    gen_min_span_tree,
-                    core_dist_n_jobs,
-                    **kwargs
-                )
+            func = _hdbscan_prims
         else:  # Metric is a valid BallTree metric
-            # TO DO: Need heuristic to decide when to go to boruvka;
-            # still debugging for now
-            if X.shape[1] > 60:
-                (single_linkage_tree, result_min_span_tree) = memory.cache(
-                    _hdbscan_prims_balltree
-                )(
-                    X,
-                    min_samples,
-                    alpha,
-                    metric,
-                    p,
-                    leaf_size,
-                    gen_min_span_tree,
-                    **kwargs
-                )
-            else:
-                (single_linkage_tree, result_min_span_tree) = memory.cache(
-                    _hdbscan_boruvka_balltree
-                )(
-                    X,
-                    min_samples,
-                    alpha,
-                    metric,
-                    p,
-                    leaf_size,
-                    approx_min_span_tree,
-                    gen_min_span_tree,
-                    core_dist_n_jobs,
-                    **kwargs
-                )
-
-    return (
-        _tree_to_labels(
-            X,
-            single_linkage_tree,
-            min_cluster_size,
-            cluster_selection_method,
-            allow_single_cluster,
-            match_reference_implementation,
-            cluster_selection_epsilon,
-            max_cluster_size,
-        )
-        + (result_min_span_tree,)
+            func = _hdbscan_prims
+            kwargs["algo"] = "ball_tree"
+
+    single_linkage_tree = memory.cache(func)(**kwargs)
+
+    return _tree_to_labels(
+        single_linkage_tree,
+        min_cluster_size,
+        cluster_selection_method,
+        allow_single_cluster,
+        cluster_selection_epsilon,
+        max_cluster_size,
     )
 
 
-# Inherits from sklearn
-class HDBSCAN(BaseEstimator, ClusterMixin):
+class HDBSCAN(ClusterMixin, BaseEstimator):
     """Perform HDBSCAN clustering from vector array or distance matrix.
 
     HDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications
@@ -892,105 +509,88 @@
 
     Parameters
     ----------
-    min_cluster_size : int, optional (default=5)
-        The minimum size of clusters; single linkage splits that contain
-        fewer points than this will be considered points "falling out" of a
-        cluster rather than a cluster splitting into two new clusters.
-
-    min_samples : int, optional (default=None)
-        The number of samples in a neighbourhood for a point to be
-        considered a core point.
+    min_cluster_size : int, default=5
+        The minimum number of samples in a group for that group to be
+        considered a cluster; groupings smaller than this size will be left
+        as noise.
+
+    min_samples : int, default=None
+        The number of samples in a neighborhood for a point
+        to be considered as a core point. This includes the point itself.
+        defaults to the `min_cluster_size`.
 
-    metric : string, or callable, optional (default='euclidean')
+    cluster_selection_epsilon : float, default=0.0
+        A distance threshold. Clusters below this value will be merged.
+        See [5]_ for more information.
+
+    max_cluster_size : int, default=0
+        A limit to the size of clusters returned by the `eom` cluster selection
+        algorithm. Has no effect if `cluster_selection_method=leaf`. Can be
+        overridden in rare cases by a high value for
+        `cluster_selection_epsilon`.
+
+    metric : str or callable, default='euclidean'
         The metric to use when calculating distance between instances in a
-        feature array. If metric is a string or callable, it must be one of
-        the options allowed by metrics.pairwise.pairwise_distances for its
-        metric parameter.
-        If metric is "precomputed", X is assumed to be a distance matrix and
-        must be square.
+        feature array.
 
-    p : int, optional (default=None)
-        p value to use if using the minkowski metric.
+        - If metric is a string or callable, it must be one of
+          the options allowed by `metrics.pairwise.pairwise_distances` for its
+          metric parameter.
 
-    alpha : float, optional (default=1.0)
+        - If metric is "precomputed", X is assumed to be a distance matrix and
+          must be square.
+
+    alpha : float, default=1.0
         A distance scaling parameter as used in robust single linkage.
         See [3]_ for more information.
 
-    cluster_selection_epsilon: float, optional (default=0.0)
-                A distance threshold. Clusters below this value will be merged.
-        See [5]_ for more information.
-
-    algorithm : string, optional (default='best')
+    algorithm : str, default='auto'
         Exactly which algorithm to use; hdbscan has variants specialised
         for different characteristics of the data. By default this is set
-        to ``best`` which chooses the "best" algorithm given the nature of
-        the data. You can force other options if you believe you know
-        better. Options are:
-            * ``best``
-            * ``generic``
-            * ``prims_kdtree``
-            * ``prims_balltree``
-            * ``boruvka_kdtree``
-            * ``boruvka_balltree``
-
-    leaf_size: int, optional (default=40)
-        If using a space tree algorithm (kdtree, or balltree) the number
-        of points ina leaf node of the tree. This does not alter the
-        resulting clustering, but may have an effect on the runtime
-        of the algorithm.
+        to `'auto'` which attempts to use a `KDTree` tree if possible,
+        otherwise it uses a `BallTree` tree.
+
+        If the `X` passed during `fit` is sparse or `metric` is invalid for
+        both `KDTree` and `BallTree`, then it resolves to use the `brute`
+        algorithm.
+
+        Available algorithms:
+        - `'brute'`
+        - `'kdtree'`
+        - `'balltree'`
+
+    leaf_size : int, default=40
+        Leaf size for trees responsible for fast nearest neighbour queries. A
+        large dataset size and small leaf_size may induce excessive memory
+        usage. If you are running out of memory consider increasing the
+        `leaf_size` parameter. Ignored for `algorithm=brute`.
 
-    memory : Instance of joblib.Memory or string (optional)
+    memory : str, default=None
         Used to cache the output of the computation of the tree.
         By default, no caching is done. If a string is given, it is the
         path to the caching directory.
 
-    approx_min_span_tree : bool, optional (default=True)
-        Whether to accept an only approximate minimum spanning tree.
-        For some algorithms this can provide a significant speedup, but
-        the resulting clustering may be of marginally lower quality.
-        If you are willing to sacrifice speed for correctness you may want
-        to explore this; in general this should be left at the default True.
-
-    gen_min_span_tree: bool, optional (default=False)
-        Whether to generate the minimum spanning tree with regard
-        to mutual reachability distance for later analysis.
-
-    core_dist_n_jobs : int, optional (default=4)
+    n_jobs : int, default=4
         Number of parallel jobs to run in core distance computations (if
-        supported by the specific algorithm). For ``core_dist_n_jobs``
-        below -1, (n_cpus + 1 + core_dist_n_jobs) are used.
+        supported by the specific algorithm). For `n_jobs<0`,
+        `(n_cpus + n_jobs + 1)` are used.
 
-    cluster_selection_method : string, optional (default='eom')
+    cluster_selection_method : str, default='eom'
         The method used to select clusters from the condensed tree. The
         standard approach for HDBSCAN* is to use an Excess of Mass algorithm
         to find the most persistent clusters. Alternatively you can instead
         select the clusters at the leaves of the tree -- this provides the
         most fine grained and homogeneous clusters. Options are:
-            * ``eom``
-            * ``leaf``
+        - `eom`
+        - `leaf`
 
-    allow_single_cluster : bool, optional (default=False)
+    allow_single_cluster : bool, default=False
         By default HDBSCAN* will not produce a single cluster, setting this
         to True will override this and allow single cluster results in
         the case that you feel this is a valid result for your dataset.
 
-    prediction_data : boolean, optional
-        Whether to generate extra cached data for predicting labels or
-        membership vectors few new unseen points later. If you wish to
-        persist the clustering object for later re-use you probably want
-        to set this to True.
-        (default False)
-
-    match_reference_implementation : bool, optional (default=False)
-        There exist some interpretational differences between this
-        HDBSCAN* implementation and the original authors reference
-        implementation in Java. This can result in very minor differences
-        in clustering results. Setting this flag to True will, at a some
-        performance cost, ensure that the clustering results match the
-        reference implementation.
-
-    **kwargs : optional
-        Arguments passed to the distance metric
+    metric_params : dict, default=None
+        Arguments passed to the distance metric.
 
     Attributes
     ----------
@@ -1004,57 +604,19 @@
         have values assigned proportional to the degree that they
         persist as part of the cluster.
 
-    cluster_persistence_ : ndarray, shape (n_clusters, )
-        A score of how persistent each cluster is. A score of 1.0 represents
-        a perfectly stable cluster that persists over all distance scales,
-        while a score of 0.0 represents a perfectly ephemeral cluster. These
-        scores can be guage the relative coherence of the clusters output
-        by the algorithm.
-
-    condensed_tree_ : CondensedTree object
-        The condensed tree produced by HDBSCAN. The object has methods
-        for converting to pandas, networkx, and plotting.
-
-    single_linkage_tree_ : SingleLinkageTree object
-        The single linkage tree produced by HDBSCAN. The object has methods
-        for converting to pandas, networkx, and plotting.
-
-    minimum_spanning_tree_ : MinimumSpanningTree object
-        The minimum spanning tree of the mutual reachability graph generated
-        by HDBSCAN. Note that this is not generated by default and will only
-        be available if `gen_min_span_tree` was set to True on object creation.
-        Even then in some optimized cases a tre may not be generated.
-
-    outlier_scores_ : ndarray, shape (n_samples, )
-        Outlier scores for clustered points; the larger the score the more
-        outlier-like the point. Useful as an outlier detection technique.
-        Based on the GLOSH algorithm by Campello, Moulavi, Zimek and Sander.
-
-    prediction_data_ : PredictionData object
-        Cached data used for predicting the cluster labels of new or
-        unseen points. Necessary only if you are using functions from
-        ``hdbscan.prediction`` (see
-        :func:`~hdbscan.prediction.approximate_predict`,
-        :func:`~hdbscan.prediction.membership_vector`,
-        and :func:`~hdbscan.prediction.all_points_membership_vectors`).
-
-    exemplars_ : list
-        A list of exemplar points for clusters. Since HDBSCAN supports
-        arbitrary shapes for clusters we cannot provide a single cluster
-        exemplar per cluster. Instead a list is returned with each element
-        of the list being a numpy array of exemplar points for a cluster --
-        these points are the "most representative" points of the cluster.
-
-    relative_validity_ : float
-        A fast approximation of the Density Based Cluster Validity (DBCV)
-        score [4]. The only differece, and the speed, comes from the fact
-        that this relative_validity_ is computed using the mutual-
-        reachability minimum spanning tree, i.e. minimum_spanning_tree_,
-        instead of the all-points minimum spanning tree used in the
-        reference. This score might not be an objective measure of the
-        goodness of clusterering. It may only be used to compare results
-        across different choices of hyper-parameters, therefore is only a
-        relative score.
+    n_features_in_ : int
+        Number of features seen during :term:`fit`.
+
+    feature_names_in_ : ndarray of shape (`n_features_in_`,)
+        Names of features seen during :term:`fit`. Defined only when `X`
+        has feature names that are all strings.
+
+    See Also
+    --------
+    DBSCAN : Density-Based Spatial Clustering of Applications
+        with Noise.
+    OPTICS : Ordering Points To Identify the Clustering Structure.
+    BIRCH : Memory-efficient, online-learning algorithm.
 
     References
     ----------
@@ -1080,8 +642,20 @@
     .. [5] Malzer, C., & Baum, M. (2019). A Hybrid Approach To Hierarchical
            Density-based Cluster Selection. arxiv preprint 1911.02282.
 
+    Examples
+    --------
+    >>> from sklearn.cluster import HDBSCAN
+    >>> from sklearn.datasets import load_digits
+    >>> X, _ = load_digits(return_X_y=True)
+    >>> hdb = HDBSCAN(min_cluster_size=20)
+    >>> hdb.fit(X)
+    HDBSCAN(min_cluster_size=20)
+    >>> hdb.labels_
+    array([ 2,  6, -1, ..., -1, -1, -1])
     """
 
+    _parameter_constraints = _PARAM_CONSTRAINTS
+
     def __init__(
         self,
         min_cluster_size=5,
@@ -1090,18 +664,13 @@
         max_cluster_size=0,
         metric="euclidean",
         alpha=1.0,
-        p=None,
-        algorithm="best",
+        algorithm="auto",
         leaf_size=40,
-        memory=Memory(cachedir=None, verbose=0),
-        approx_min_span_tree=True,
-        gen_min_span_tree=False,
-        core_dist_n_jobs=4,
+        memory=None,
+        n_jobs=4,
         cluster_selection_method="eom",
         allow_single_cluster=False,
-        prediction_data=False,
-        match_reference_implementation=False,
-        **kwargs
+        metric_params=None,
     ):
         self.min_cluster_size = min_cluster_size
         self.min_samples = min_samples
@@ -1109,27 +678,13 @@
         self.max_cluster_size = max_cluster_size
         self.cluster_selection_epsilon = cluster_selection_epsilon
         self.metric = metric
-        self.p = p
         self.algorithm = algorithm
         self.leaf_size = leaf_size
         self.memory = memory
-        self.approx_min_span_tree = approx_min_span_tree
-        self.gen_min_span_tree = gen_min_span_tree
-        self.core_dist_n_jobs = core_dist_n_jobs
+        self.n_jobs = n_jobs
         self.cluster_selection_method = cluster_selection_method
         self.allow_single_cluster = allow_single_cluster
-        self.match_reference_implementation = match_reference_implementation
-        self.prediction_data = prediction_data
-
-        self._metric_kwargs = kwargs
-
-        self._condensed_tree = None
-        self._single_linkage_tree = None
-        self._min_spanning_tree = None
-        self._raw_data = None
-        self._outlier_scores = None
-        self._prediction_data = None
-        self._relative_validity = None
+        self.metric_params = metric_params
 
     def fit(self, X, y=None):
         """Perform HDBSCAN clustering from features or distance matrix.
@@ -1139,143 +694,110 @@
         X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
                 array of shape (n_samples, n_samples)
             A feature array, or array of distances between samples if
-            ``metric='precomputed'``.
+            `metric='precomputed'`.
+
+        y : Ignored
+            Ignored.
 
         Returns
         -------
         self : object
-            Returns self
+            Returns self.
         """
+        self._validate_params()
+        metric_params = self.metric_params or {}
         if self.metric != "precomputed":
             # Non-precomputed matrices may contain non-finite values.
             # Rows with these values
-            X = check_array(X, accept_sparse="csr", force_all_finite=False)
+            X = self._validate_data(X, force_all_finite=False, accept_sparse="csr")
             self._raw_data = X
 
-            self._all_finite = is_finite(X)
-            if ~self._all_finite:
+            self._all_finite = (
+                np.all(np.isfinite(X.data)) if issparse(X) else np.all(np.isfinite(X))
+            )
+
+            if not self._all_finite:
                 # Pass only the purely finite indices into hdbscan
-                # We will later assign all non-finite points to the background -1 cluster
+                # We will later assign all non-finite points to the
+                # background-1 cluster
                 finite_index = get_finite_row_indices(X)
-                clean_data = X[finite_index]
-                internal_to_raw = {
-                    x: y for x, y in zip(range(len(finite_index)), finite_index)
-                }
+                X = X[finite_index]
+                internal_to_raw = {x: y for x, y in enumerate(finite_index)}
                 outliers = list(set(range(X.shape[0])) - set(finite_index))
-            else:
-                clean_data = X
         elif issparse(X):
             # Handle sparse precomputed distance matrices separately
-            X = check_array(X, accept_sparse="csr")
-            clean_data = X
+            X = self._validate_data(X, accept_sparse="csr")
         else:
             # Only non-sparse, precomputed distance matrices are allowed
             #   to have numpy.inf values indicating missing distances
-            check_precomputed_distance_matrix(X)
-            clean_data = X
+            X = self._validate_data(X, force_all_finite="allow-nan")
 
+        self.n_features_in_ = X.shape[1]
         kwargs = self.get_params()
         # prediction data only applies to the persistent model, so remove
         # it from the keyword args we pass on the the function
-        kwargs.pop("prediction_data", None)
-        kwargs.update(self._metric_kwargs)
+        kwargs["metric_params"] = metric_params
 
         (
             self.labels_,
             self.probabilities_,
-            self.cluster_persistence_,
-            self._condensed_tree,
-            self._single_linkage_tree,
-            self._min_spanning_tree,
-        ) = hdbscan(clean_data, **kwargs)
+            self._single_linkage_tree_,
+        ) = hdbscan(X, **kwargs)
 
         if self.metric != "precomputed" and not self._all_finite:
-            # remap indices to align with original data in the case of non-finite entries.
-            self._condensed_tree = remap_condensed_tree(
-                self._condensed_tree, internal_to_raw, outliers
+            # remap indices to align with original data in the case of
+            # non-finite entries.
+            self._single_linkage_tree_ = remap_single_linkage_tree(
+                self._single_linkage_tree_, internal_to_raw, outliers
             )
-            self._single_linkage_tree = remap_single_linkage_tree(
-                self._single_linkage_tree, internal_to_raw, outliers
-            )
-            new_labels = np.full(X.shape[0], -1)
+            new_labels = np.full(self._raw_data.shape[0], -1)
             new_labels[finite_index] = self.labels_
             self.labels_ = new_labels
 
-            new_probabilities = np.zeros(X.shape[0])
+            new_probabilities = np.zeros(self._raw_data.shape[0])
             new_probabilities[finite_index] = self.probabilities_
             self.probabilities_ = new_probabilities
 
-        if self.prediction_data:
-            self.generate_prediction_data()
-
         return self
 
     def fit_predict(self, X, y=None):
-        """Performs clustering on X and returns cluster labels.
+        """Perform clustering on X and return cluster labels.
 
         Parameters
         ----------
         X : array or sparse (CSR) matrix of shape (n_samples, n_features), or \
                 array of shape (n_samples, n_samples)
             A feature array, or array of distances between samples if
-            ``metric='precomputed'``.
+            `metric='precomputed'`.
+
+        y : Ignored
+            Ignored.
 
         Returns
         -------
         y : ndarray, shape (n_samples, )
-            cluster labels
+            Cluster labels.
         """
         self.fit(X)
         return self.labels_
 
-    def generate_prediction_data(self):
-        """
-        Create data that caches intermediate results used for predicting
-        the label of new/unseen points. This data is only useful if
-        you are intending to use functions from ``hdbscan.prediction``.
+    def weighted_cluster_centroid(self, cluster_id):
         """
+        Provide an approximate representative point for a given cluster.
 
-        if self.metric in FAST_METRICS:
-            min_samples = self.min_samples or self.min_cluster_size
-            if self.metric in KDTree.valid_metrics:
-                tree_type = "kdtree"
-            elif self.metric in BallTree.valid_metrics:
-                tree_type = "balltree"
-            else:
-                warn("Metric {} not supported for prediction data!".format(self.metric))
-                return
-
-            self._prediction_data = PredictionData(
-                self._raw_data,
-                self.condensed_tree_,
-                min_samples,
-                tree_type=tree_type,
-                metric=self.metric,
-                **self._metric_kwargs
-            )
-        else:
-            warn(
-                "Cannot generate prediction data for non-vector"
-                "space inputs -- access to the source data rather"
-                "than mere distances is required!"
-            )
-
-    def weighted_cluster_centroid(self, cluster_id):
-        """Provide an approximate representative point for a given cluster.
         Note that this technique assumes a euclidean metric for speed of
-        computation. For more general metrics use the ``weighted_cluster_medoid``
-        method which is slower, but can work with the metric the model trained
-        with.
+        computation. For more general metrics use the `weighted_cluster_medoid`
+        method which is slower, but can work with more general metrics.
 
         Parameters
         ----------
-        cluster_id: int
+        cluster_id : int
             The id of the cluster to compute a centroid for.
 
         Returns
         -------
-        centroid: array of shape (n_features,)
-            A representative centroid for cluster ``cluster_id``.
+        centroid : array of shape (n_features,)
+            A representative centroid for cluster `cluster_id`.
         """
         if not hasattr(self, "labels_"):
             raise AttributeError("Model has not been fit to data")
@@ -1293,20 +815,22 @@
         return np.average(cluster_data, weights=cluster_membership_strengths, axis=0)
 
     def weighted_cluster_medoid(self, cluster_id):
-        """Provide an approximate representative point for a given cluster.
+        """
+        Provide an approximate representative point for a given cluster.
+
         Note that this technique can be very slow and memory intensive for
-        large clusters. For faster results use the ``weighted_cluster_centroid``
+        large clusters. For faster results use the `weighted_cluster_centroid`
         method which is faster, but assumes a euclidean metric.
 
         Parameters
         ----------
-        cluster_id: int
+        cluster_id : int
             The id of the cluster to compute a medoid for.
 
         Returns
         -------
-        centroid: array of shape (n_features,)
-            A representative medoid for cluster ``cluster_id``.
+        centroid : array of shape (n_features,)
+            A representative medoid for cluster `cluster_id`.
         """
         if not hasattr(self, "labels_"):
             raise AttributeError("Model has not been fit to data")
@@ -1320,22 +844,26 @@
         mask = self.labels_ == cluster_id
         cluster_data = self._raw_data[mask]
         cluster_membership_strengths = self.probabilities_[mask]
+        metric_params = self.metric_params or {}
 
-        dist_mat = pairwise_distances(
-            cluster_data, metric=self.metric, **self._metric_kwargs
-        )
+        dist_mat = pairwise_distances(cluster_data, metric=self.metric, **metric_params)
 
         dist_mat = dist_mat * cluster_membership_strengths
         medoid_index = np.argmin(dist_mat.sum(axis=1))
         return cluster_data[medoid_index]
 
     def dbscan_clustering(self, cut_distance, min_cluster_size=5):
-        """Return clustering that would be equivalent to running DBSCAN* for a particular cut_distance (or epsilon)
-        DBSCAN* can be thought of as DBSCAN without the border points.  As such these results may differ slightly
-        from sklearns implementation of dbscan in the non-core points.
+        """
+        Return clustering given by DBSCAN without border points.
+
+        Return clustering that would be equivalent to running DBSCAN* for a
+        particular cut_distance (or epsilon) DBSCAN* can be thought of as
+        DBSCAN without the border points.  As such these results may differ
+        slightly from `cluster.DBSCAN` due to the difference in implementation
+        over the non-core points.
 
-        This can also be thought of as a flat clustering derived from constant height cut through the single
-        linkage tree.
+        This can also be thought of as a flat clustering derived from constant
+        height cut through the single linkage tree.
 
         This represents the result of selecting a cut value for robust single linkage
         clustering. The `min_cluster_size` allows the flat clustering to declare noise
@@ -1345,181 +873,23 @@
         ----------
 
         cut_distance : float
-            The mutual reachability distance cut value to use to generate a flat clustering.
+            The mutual reachability distance cut value to use to generate a
+            flat clustering.
 
         min_cluster_size : int, optional
-            Clusters smaller than this value with be called 'noise' and remain unclustered
-            in the resulting flat clustering.
+            Clusters smaller than this value with be called 'noise' and remain
+            unclustered in the resulting flat clustering.
 
         Returns
         -------
 
         labels : array [n_samples]
-            An array of cluster labels, one per datapoint. Unclustered points are assigned
-            the label -1.
+            An array of cluster labels, one per datapoint. Unclustered points
+            are assigned the label -1.
         """
-        return self.single_linkage_tree_.get_clusters(
-            cut_distance=cut_distance,
-            min_cluster_size=min_cluster_size,
+        return labelling_at_cut(
+            self._single_linkage_tree_, cut_distance, min_cluster_size
         )
 
-    @property
-    def prediction_data_(self):
-        if self._prediction_data is None:
-            raise AttributeError("No prediction data was generated")
-        else:
-            return self._prediction_data
-
-    @property
-    def outlier_scores_(self):
-        if self._outlier_scores is not None:
-            return self._outlier_scores
-        else:
-            if self._condensed_tree is not None:
-                self._outlier_scores = outlier_scores(self._condensed_tree)
-                return self._outlier_scores
-            else:
-                raise AttributeError(
-                    "No condensed tree was generated; try running fit first."
-                )
-
-    @property
-    def condensed_tree_(self):
-        if self._condensed_tree is not None:
-            return CondensedTree(
-                self._condensed_tree,
-                self.cluster_selection_method,
-                self.allow_single_cluster,
-            )
-        else:
-            raise AttributeError(
-                "No condensed tree was generated; try running fit first."
-            )
-
-    @property
-    def single_linkage_tree_(self):
-        if self._single_linkage_tree is not None:
-            return SingleLinkageTree(self._single_linkage_tree)
-        else:
-            raise AttributeError(
-                "No single linkage tree was generated; try running fit" " first."
-            )
-
-    @property
-    def minimum_spanning_tree_(self):
-        if self._min_spanning_tree is not None:
-            if self._raw_data is not None:
-                return MinimumSpanningTree(self._min_spanning_tree, self._raw_data)
-            else:
-                warn(
-                    "No raw data is available; this may be due to using"
-                    " a precomputed metric matrix. No minimum spanning"
-                    " tree will be provided without raw data."
-                )
-                return None
-        else:
-            raise AttributeError(
-                "No minimum spanning tree was generated."
-                "This may be due to optimized algorithm variations that skip"
-                " explicit generation of the spanning tree."
-            )
-
-    @property
-    def exemplars_(self):
-        if self._prediction_data is not None:
-            return self._prediction_data.exemplars
-        elif self.metric in FAST_METRICS:
-            self.generate_prediction_data()
-            return self._prediction_data.exemplars
-        else:
-            raise AttributeError(
-                "Currently exemplars require the use of vector input data"
-                "with a suitable metric. This will likely change in the "
-                "future, but for now no exemplars can be provided"
-            )
-
-    @property
-    def relative_validity_(self):
-        if self._relative_validity is not None:
-            return self._relative_validity
-
-        if not self.gen_min_span_tree:
-            raise AttributeError(
-                "Minimum spanning tree not present. "
-                + "Either HDBSCAN object was created with "
-                + "gen_min_span_tree=False or the tree was "
-                + "not generated in spite of it owing to "
-                + "internal optimization criteria."
-            )
-            return
-
-        labels = self.labels_
-        sizes = np.bincount(labels + 1)
-        noise_size = sizes[0]
-        cluster_size = sizes[1:]
-        total = noise_size + np.sum(cluster_size)
-        num_clusters = len(cluster_size)
-        DSC = np.zeros(num_clusters)
-        min_outlier_sep = np.inf  # only required if num_clusters = 1
-        correction_const = 2  # only required if num_clusters = 1
-
-        # Unltimately, for each Ci, we only require the
-        # minimum of DSPC(Ci, Cj) over all Cj != Ci.
-        # So let's call this value DSPC_wrt(Ci), i.e.
-        # density separation 'with respect to' Ci.
-        DSPC_wrt = np.ones(num_clusters) * np.inf
-        max_distance = 0
-
-        mst_df = self.minimum_spanning_tree_.to_pandas()
-
-        for edge in mst_df.iterrows():
-            label1 = labels[int(edge[1]["from"])]
-            label2 = labels[int(edge[1]["to"])]
-            length = edge[1]["distance"]
-
-            max_distance = max(max_distance, length)
-
-            if label1 == -1 and label2 == -1:
-                continue
-            elif label1 == -1 or label2 == -1:
-                # If exactly one of the points is noise
-                min_outlier_sep = min(min_outlier_sep, length)
-                continue
-
-            if label1 == label2:
-                # Set the density sparseness of the cluster
-                # to the sparsest value seen so far.
-                DSC[label1] = max(length, DSC[label1])
-            else:
-                # Check whether density separations with
-                # respect to each of these clusters can
-                # be reduced.
-                DSPC_wrt[label1] = min(length, DSPC_wrt[label1])
-                DSPC_wrt[label2] = min(length, DSPC_wrt[label2])
-
-        # In case min_outlier_sep is still np.inf, we assign a new value to it.
-        # This only makes sense if num_clusters = 1 since it has turned out
-        # that the MR-MST has no edges between a noise point and a core point.
-        min_outlier_sep = max_distance if min_outlier_sep == np.inf else min_outlier_sep
-
-        # DSPC_wrt[Ci] might be infinite if the connected component for Ci is
-        # an "island" in the MR-MST. Whereas for other clusters Cj and Ck, the
-        # MR-MST might contain an edge with one point in Cj and ther other one
-        # in Ck. Here, we replace the infinite density separation of Ci by
-        # another large enough value.
-        #
-        # TODO: Think of a better yet efficient way to handle this.
-        correction = correction_const * (
-            max_distance if num_clusters > 1 else min_outlier_sep
-        )
-        DSPC_wrt[np.where(DSPC_wrt == np.inf)] = correction
-
-        V_index = [
-            (DSPC_wrt[i] - DSC[i]) / max(DSPC_wrt[i], DSC[i])
-            for i in range(num_clusters)
-        ]
-        score = np.sum(
-            [(cluster_size[i] * V_index[i]) / total for i in range(num_clusters)]
-        )
-        self._relative_validity = score
-        return self._relative_validity
+    def _more_tags(self):
+        return {"allow_nan": True}
